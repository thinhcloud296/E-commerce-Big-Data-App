# Big Data App cho dataset: E-commerce Customer Behavior and Purchase
# - ƒê·ªçc CSV t·ª´ HDFS: hdfs://namenode:8020/input/ecommerce_data.csv
# - ETL -> Parquet (partition theo PurchaseDate)
# - Analytics: KPI, Top Category/Payment, Revenue theo ng√†y
# - ML: Logistic Regression d·ª± ƒëo√°n Churn; KMeans ph√¢n c·ª•m kh√°ch h√†ng (RFM ƒë∆°n gi·∫£n)
#
# Y√™u c·∫ßu m√¥i tr∆∞·ªùng:
#   - Spark master: spark://spark-master:7077 (c√≥ th·ªÉ ƒë·ªïi trong sidebar)
#   - HDFS: namenode:8020
#   - pip install streamlit pyspark pandas matplotlib

import os
import traceback
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType
)
from pyspark.sql.functions import (
    col, to_timestamp, date_format, coalesce, trim, regexp_replace,
    countDistinct, count, sum as _sum, avg, expr, when, max as _max
)



# =========================
# Config m·∫∑c ƒë·ªãnh
# =========================
DEFAULT_SPARK_MASTER = os.getenv("SPARK_MASTER_URL", os.getenv("SPARK_MASTER", "spark://spark-master:7077"))
DEFAULT_INPUT = "hdfs://namenode:8020/input/ecommerce_data.csv"
DEFAULT_WAREHOUSE = os.getenv("WAREHOUSE_PATH", "hdfs://namenode:8020/warehouse")
DEFAULT_OUTPUT = os.getenv("OUTPUT_PATH", "hdfs://namenode:8020/output")
PARQUET_DIR = f"{DEFAULT_WAREHOUSE.rstrip('/')}/ecommerce_parquet"

# =========================
# Spark helpers (1 version only)
# =========================
# ---- CACHE: ƒë·ªçc Parquet 1 l·∫ßn cho c·∫£ Analytics & ML ----
def add_churn_from_recency(df, customer_col="customer_id", ts_col="purchase_ts", cutoff_days=180):
    """
    T·∫°o c·ªôt churn d·ª±a tr√™n 'recency':
      - T√¨m l·∫ßn mua g·∫ßn nh·∫•t c·ªßa t·ª´ng kh√°ch h√†ng.
      - N·∫øu s·ªë ng√†y t·ª´ l·∫ßn mua g·∫ßn nh·∫•t t·ªõi m·ªëc th·ªùi gian max trong dataset > cutoff_days => churn=1, ng∆∞·ª£c l·∫°i 0.
    Tr·∫£ v·ªÅ DataFrame ƒë√£ c√≥ c·ªôt 'churn' (int).
    """
    mx = df.agg(_max(ts_col).alias("mx")).collect()[0]["mx"]
    if mx is None:
        # Kh√¥ng c√≥ timestamp h·ª£p l·ªá -> kh√¥ng th·ªÉ t√≠nh recency: set churn=0 n·∫øu ch∆∞a c√≥
        return df.withColumn("churn", when(col("churn").isNull(), 0).otherwise(col("churn")).cast("int"))

    last_by_cust = (
        df.groupBy(customer_col)
          .agg(_max(ts_col).alias("last_purchase"))
          .withColumn("days_since_last", expr(f"datediff(to_timestamp('{str(mx)}'), last_purchase)"))
          .withColumn("churn_calc", (col("days_since_last") > cutoff_days).cast("int"))
          .select(customer_col, "churn_calc")
    )

    df2 = (
        df.join(last_by_cust, on=customer_col, how="left")
          .withColumn("churn", coalesce(col("churn").cast("int"), col("churn_calc")).cast("int"))
          .drop("churn_calc")
    )
    return df2
@st.cache_resource(show_spinner=False)
def load_parquet_cached(app_id: str, parquet_path: str):
    """
    Cache DataFrame theo app_id hi·ªán t·∫°i ƒë·ªÉ tr√°nh gi·ªØ DF g·∫Øn v·ªõi SparkSession ƒë√£ stop.
    """
    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    df = spark.read.parquet(parquet_path).cache()   # cache tr√™n executors
    _ = df.count()                                   # materialize ƒë·ªÉ l·∫ßn sau r·∫•t nhanh
    return df


def _spark_is_alive(spark):
    try:
        return spark is not None and not spark.sparkContext._jsc.sc().isStopped()
    except Exception:
        return False

def _hard_reset_spark_jvm():
    """Xo√° session m·∫∑c ƒë·ªãnh/active v√† stop SparkContext c√≤n treo trong JVM."""
    try:
        # clear default/active SparkSession (t·∫ßng JVM)
        SparkSession.clearActiveSession()
        SparkSession.clearDefaultSession()
    except Exception:
        pass
    try:
        # stop SparkContext ƒëang active n·∫øu c√≤n
        from pyspark import SparkContext
        sc = SparkContext._active_spark_context
        if sc is not None:
            sc.stop()
    except Exception:
        pass

def start_spark(app_name: str = "EcomBigDataApp", master: str = DEFAULT_SPARK_MASTER, log_level: str = "WARN"):
    # L·∫ßn 1: th·ª≠ t·∫°o b√¨nh th∆∞·ªùng
    try:
        spark = (SparkSession.builder
                 .appName(app_name)
                 .master(master)
                 .config("spark.sql.shuffle.partitions", "64")
                 .config("spark.ui.showConsoleProgress", "true")
                 .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                 .getOrCreate())
        if not _spark_is_alive(spark):
            raise RuntimeError("SparkContext stopped right after getOrCreate()")
        spark.sparkContext.setLogLevel(log_level)
        return spark
    except Exception as e1:
        print("[WARN] First attempt failed ‚Üí reset JVM sessions then retry. Reason:", repr(e1))
        # D·ªåN S·∫†CH r·ªìi th·ª≠ l·∫°i v·ªõi master th·∫≠t
        _hard_reset_spark_jvm()
        try:
            spark = (SparkSession.builder
                     .appName(app_name)
                     .master(master)
                     .config("spark.sql.shuffle.partitions", "64")
                     .config("spark.ui.showConsoleProgress", "true")
                     .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                     .getOrCreate())
            if not _spark_is_alive(spark):
                raise RuntimeError("SparkContext stopped again after getOrCreate()")
            spark.sparkContext.setLogLevel(log_level)
            return spark
        except Exception as e2:
            print("[WARN] Second attempt with master failed ‚Üí fallback local[*]. Reason:", repr(e2))
            _hard_reset_spark_jvm()
            spark = (SparkSession.builder
                     .appName(app_name + "-local")
                     .master("local[*]")
                     .config("spark.sql.shuffle.partitions", "8")
                     .config("spark.ui.showConsoleProgress", "true")
                     .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                     .getOrCreate())
            spark.sparkContext.setLogLevel(log_level)
            return spark
def ensure_spark(master: str, app_name: str = "EcomBigDataApp", log_level: str = "WARN"):
    spark = st.session_state.get("spark")
    if not _spark_is_alive(spark):
        _hard_reset_spark_jvm()
        spark = start_spark(app_name=app_name, master=master, log_level=log_level)
        st.session_state["spark"] = spark
    return spark


# =========================
# Schema dataset theo m√¥ t·∫£
# =========================
SCHEMA = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Customer Name", StringType(), True),
    StructField("Customer Age", IntegerType(), True),
    StructField("Gender", StringType(), True),
    StructField("Purchase Date", StringType(), True),          # parse sau
    StructField("Product Category", StringType(), True),
    StructField("Product Price", DoubleType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("Total Purchase Amount", DoubleType(), True),
    StructField("Payment Method", StringType(), True),
    StructField("Returns", IntegerType(), True),               # 0/1
    StructField("Churn", IntegerType(), True)                  # 0/1
])

# =========================
# ETL
# =========================
def run_etl(spark, src_path: str, dst_parquet: str, ts_format: str):
    st.info(f"ƒê·ªçc CSV t·ª´: {src_path}")

    # ƒê·ªçc theo schema ƒë·ªãnh nghƒ©a s·∫µn (SCHEMA / SCHEMA_WITH_CUSTOMER)
    df = (
        spark.read
        .option("header", "true")
        .option("inferSchema", "true")  
        .csv(src_path)
    )

    # --- ƒê·ªïi t√™n c·ªôt v·ªÅ schema ng·∫Øn g·ªçn/th·ªëng nh·∫•t ---
    rename_pairs = {
        "Customer ID": "customer_id",
        "Customer": "customer_id",                  # h·ªó tr·ª£ header "Customer"
        "Customer Name": "customer_name",
        "Customer Age": "customer_age",
        "Gender": "gender",
        "Purchase Date": "purchase_date_raw",
        "Product Category": "product_category",
        "Product Price": "product_price",
        "Quantity": "quantity",
        "Total Purchase Amount": "total_amount",
        "Payment Method": "payment_method",
        "Returns": "returns",
        "Churn": "churn",
    }
    for old, new in rename_pairs.items():
        if old in df.columns and new not in df.columns:
            df = df.withColumnRenamed(old, new)

    # --- L√†m s·∫°ch s·ªë & √©p ki·ªÉu an to√†n ---
    for num_col in ["product_price", "total_amount"]:
        if num_col in df.columns:
            df = df.withColumn(
                num_col,
                regexp_replace(col(num_col).cast("string"), r"[^0-9\.\-]", "").cast("double")
            )
    if "quantity" in df.columns:
        df = df.withColumn("quantity",
            regexp_replace(col("quantity").cast("string"), r"[^0-9\-]", "").cast("int")
        )
    if "returns" in df.columns:
        df = df.withColumn("returns", col("returns").cast("int"))
    if "churn" in df.columns:
        df = df.withColumn("churn", col("churn").cast("int"))

    # --- Parse th·ªùi gian ƒëa ƒë·ªãnh d·∫°ng ---
    raw = "purchase_date_raw"
    if raw in df.columns:
        df = df.withColumn(raw, trim(col(raw)))
        df = df.withColumn(raw, regexp_replace(col(raw), "T", " "))
        df = df.withColumn(raw, regexp_replace(col(raw), "Z$", ""))
        df = df.withColumn(raw, regexp_replace(col(raw), r"[\u00A0\u2007\u202F]", " "))
        df = df.withColumn(raw, regexp_replace(col(raw), r"\s+", " "))

        patterns = []
        if ts_format:
            patterns.append(ts_format)

        # ∆Øu ti√™n d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n: "9/8/2020 9:38" (kh√¥ng gi√¢y, 1 ch·ªØ s·ªë)
        patterns += [
            "M/d/yyyy H:mm",
            "M/d/yyyy h:mm a",
            "MM/dd/yyyy HH:mm",
            "dd/M/yyyy H:mm",
            "dd/MM/yyyy HH:mm",
            "M/d/yyyy H:mm:ss",
            "MM/dd/yyyy HH:mm:ss",
            "dd/M/yyyy H:mm:ss",
            "dd/MM/yyyy HH:mm:ss",
            "yyyy-MM-dd",
            "MM/dd/yyyy",
            "dd/MM/yyyy",
        ]

        ts_exprs = [to_timestamp(col(raw), p) for p in patterns]
        df = df.withColumn("purchase_ts", coalesce(*ts_exprs))
        df = df.withColumn("purchase_date", date_format(col("purchase_ts"), "yyyy-MM-dd"))

        # C·∫£nh b√°o n·∫øu g·∫ßn nh∆∞ kh√¥ng parse ƒë∆∞·ª£c
        null_ratio = df.select((col("purchase_ts").isNull()).cast("int").alias("isnull")) \
                       .agg({"isnull": "avg"}).collect()[0][0]
        if null_ratio and null_ratio >= 0.9999:
            st.warning(
                "Kh√¥ng parse ƒë∆∞·ª£c c·ªôt th·ªùi gian 'Purchase Date'. "
                "H√£y nh·∫≠p ƒë√∫ng ƒë·ªãnh d·∫°ng ·ªü √¥ 'ƒê·ªãnh d·∫°ng th·ªùi gian' (v√≠ d·ª•: M/d/yyyy H:mm). "
                "App v·∫´n ghi Parquet nh∆∞ng c√°c bi·ªÉu ƒë·ªì theo ng√†y s·∫Ω r·ªóng."
            )
    else:
        df = df.withColumn("purchase_ts", None).withColumn("purchase_date", None)

    # --- B·ªï khuy·∫øt churn theo recency n·∫øu thi·∫øu ho·∫∑c ph·∫ßn l·ªõn NULL ---
    needs_fill = True
    if "churn" in df.columns:
        null_ratio = df.select((col("churn").isNull()).cast("int").alias("isnull")) \
                       .agg({"isnull": "avg"}).collect()[0][0]
        needs_fill = (null_ratio is None) or (null_ratio > 0.5)

    cutoff_days = 180  # c√≥ th·ªÉ ƒë∆∞a ra sidebar n·∫øu mu·ªën ƒëi·ªÅu ch·ªânh ƒë·ªông
    if needs_fill:
        df = add_churn_from_recency(df, customer_col="customer_id",
                                    ts_col="purchase_ts", cutoff_days=cutoff_days)

    # --- ƒêi·ªÅn NA cho m·ªôt s·ªë c·ªôt ph√¢n lo·∫°i ---
    for c in ["gender", "product_category", "payment_method"]:
        if c in df.columns:
            df = df.fillna({c: "unknown"})

    # --- Ghi Parquet (overwrite), partition theo ng√†y n·∫øu c√≥ ---
    writer = df.write.mode("overwrite")
    if "purchase_date" in df.columns:
        writer = writer.partitionBy("purchase_date")
    writer.parquet(dst_parquet)

    st.success(f"ETL ho√†n t·∫•t ‚Üí {dst_parquet}")
    return dst_parquet

# =========================
# Analytics
# =========================
def run_analytics(spark, parquet_path: str):
    st.subheader("Analytics")
    if not _spark_is_alive(spark):
        sparkContext = ensure_spark(master)  # ho·∫∑c raise l·ªói tu·ª≥ b·∫°n ƒë√£ vi·∫øt
        spark = ensure_spark(master)
    app_id = spark.sparkContext.applicationId
    df = load_parquet_cached(app_id, parquet_path)

    # T·ªïng quan
    total_rows = df.count()
    n_cols = len(df.columns)
    st.write(f"**S·ªë d√≤ng:** {total_rows:,} | **S·ªë c·ªôt:** {n_cols}")
    with st.expander("Danh s√°ch c·ªôt"):
        st.code(", ".join(df.columns))

    # ‚Äî‚Äî KPI ‚Äî‚Äî
    st.markdown("### KPIs t·ªïng quan")
    kpi = {}
    if "customer_id" in df.columns:
        kpi["S·ªë kh√°ch h√†ng"] = df.select(countDistinct("customer_id").alias("n")).collect()[0]["n"]
    if "product_category" in df.columns:
        kpi["S·ªë category"] = df.select(countDistinct("product_category").alias("n")).collect()[0]["n"]
    if "total_amount" in df.columns:
        kpi["T·ªïng doanh thu"] = df.agg(_sum("total_amount").alias("rev")).collect()[0]["rev"]
    if "returns" in df.columns:
        kpi["T·ªâ l·ªá tr·∫£ h√†ng (%)"] = round((df.agg(avg(col("returns").cast("double")).alias("r"))
                                             .collect()[0]["r"] or 0) * 100, 2)
    if "churn" in df.columns:
        kpi["T·ªâ l·ªá churn (%)"] = round((df.agg(avg(col("churn").cast("double")).alias("c"))
                                         .collect()[0]["c"] or 0) * 100, 2)

    if kpi:
        for k, v in kpi.items():
            st.write(f"- **{k}:** {v}")
    else:
        st.write("Kh√¥ng ƒë·ªß c·ªôt ƒë·ªÉ t√≠nh KPI.")

    # ‚Äî‚Äî Revenue theo ng√†y ‚Äî‚Äî
    if "purchase_date" in df.columns and "total_amount" in df.columns:
        st.markdown("### Doanh thu theo ng√†y")
        rev_day = (df.groupBy("purchase_date")
                     .agg(_sum("total_amount").alias("revenue"),
                          count("*").alias("orders"))
                     .orderBy("purchase_date"))
        pdf = rev_day.toPandas()

        st.dataframe(pdf, use_container_width=True)

        if not pdf.empty and pdf["purchase_date"].notna().any():
            fig, ax = plt.subplots()
            ax.plot(pdf["purchase_date"].astype(str), pdf["revenue"])
            ax.set_title("Revenue theo ng√†y")
            ax.set_xlabel("Ng√†y"); ax.set_ylabel("Revenue")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig)

            # Orders theo ng√†y
            fig2, ax2 = plt.subplots()
            ax2.plot(pdf["purchase_date"].astype(str), pdf["orders"])
            ax2.set_title("S·ªë ƒë∆°n theo ng√†y")
            ax2.set_xlabel("Ng√†y"); ax2.set_ylabel("Orders")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig2)

            # AOV theo ng√†y (Average Order Value)
            pdf["aov"] = pdf["revenue"] / pdf["orders"].replace(0, float("nan"))
            fig3, ax3 = plt.subplots()
            ax3.plot(pdf["purchase_date"].astype(str), pdf["aov"])
            ax3.set_title("AOV theo ng√†y")
            ax3.set_xlabel("Ng√†y"); ax3.set_ylabel("AOV")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig3)
        else:
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ng√†y (c√≥ th·ªÉ do kh√¥ng parse ƒë∆∞·ª£c 'Purchase Date').")

    # ‚Äî‚Äî Top Category theo s·ªë ƒë∆°n & doanh thu ‚Äî‚Äî
    if "product_category" in df.columns:
        st.markdown("### Top Category")
        top_n = st.slider("S·ªë l∆∞·ª£ng Top", 5, 30, 10, key="top_cat")
        top_cat = (df.groupBy("product_category")
                     .agg(count("*").alias("cnt"), _sum("total_amount").alias("revenue"))
                     .orderBy(col("cnt").desc())
                     .limit(top_n))
        pdf = top_cat.toPandas()
        st.dataframe(pdf, use_container_width=True)

        if not pdf.empty:
            # theo s·ªë ƒë∆°n
            fig4, ax4 = plt.subplots()
            ax4.bar(pdf["product_category"].astype(str), pdf["cnt"])
            ax4.set_title(f"Top {top_n} Category (theo s·ªë ƒë∆°n)")
            ax4.set_xlabel("Category"); ax4.set_ylabel("Count")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig4)

            # theo doanh thu
            fig5, ax5 = plt.subplots()
            ax5.bar(pdf["product_category"].astype(str), pdf["revenue"])
            ax5.set_title(f"Top {top_n} Category (theo doanh thu)")
            ax5.set_xlabel("Category"); ax5.set_ylabel("Revenue")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig5)

    # ‚Äî‚Äî Ph∆∞∆°ng th·ª©c thanh to√°n ‚Äî‚Äî
    if "payment_method" in df.columns:
        st.markdown("### Ph√¢n b·ªë ph∆∞∆°ng th·ª©c thanh to√°n")
        pm = df.groupBy("payment_method").agg(count("*").alias("cnt"),
                                              _sum("total_amount").alias("revenue")) \
               .orderBy(col("cnt").desc())
        pdf = pm.toPandas()
        st.dataframe(pdf, use_container_width=True)

        if not pdf.empty:
            fig6, ax6 = plt.subplots()
            ax6.bar(pdf["payment_method"].astype(str), pdf["cnt"])
            ax6.set_title("S·ªë ƒë∆°n theo ph∆∞∆°ng th·ª©c thanh to√°n")
            ax6.set_xlabel("Payment Method"); ax6.set_ylabel("Count")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig6)

            fig7, ax7 = plt.subplots()
            ax7.bar(pdf["payment_method"].astype(str), pdf["revenue"])
            ax7.set_title("Doanh thu theo ph∆∞∆°ng th·ª©c thanh to√°n")
            ax7.set_xlabel("Payment Method"); ax7.set_ylabel("Revenue")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig7)

    # ‚Äî‚Äî Returns & Churn theo ng√†y (n·∫øu c√≥ c·ªôt) ‚Äî‚Äî
    if "purchase_date" in df.columns and "returns" in df.columns:
        st.markdown("### T·ªâ l·ªá tr·∫£ h√†ng theo ng√†y")
        rate = (df.groupBy("purchase_date")
                  .agg(avg(col("returns").cast("double")).alias("return_rate"))
                  .orderBy("purchase_date")).toPandas()
        if not rate.empty and rate["purchase_date"].notna().any():
            fig8, ax8 = plt.subplots()
            ax8.plot(rate["purchase_date"].astype(str), rate["return_rate"])
            ax8.set_title("T·ªâ l·ªá tr·∫£ h√†ng theo ng√†y")
            ax8.set_xlabel("Ng√†y"); ax8.set_ylabel("Return rate")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig8)

    if "churn" in df.columns and "gender" in df.columns:
        st.markdown("### T·ªâ l·ªá churn theo gi·ªõi t√≠nh")
        churn_g = (df.groupBy("gender")
                     .agg(avg(col("churn").cast("double")).alias("churn_rate"))
                     .orderBy("gender")).toPandas()
        if not churn_g.empty:
            fig9, ax9 = plt.subplots()
            ax9.bar(churn_g["gender"].astype(str), churn_g["churn_rate"])
            ax9.set_title("Churn rate theo gi·ªõi t√≠nh")
            ax9.set_xlabel("Gender"); ax9.set_ylabel("Churn rate")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig9)

    # ‚Äî‚Äî Ph√¢n b·ªë tu·ªïi (n·∫øu c√≥) ‚Äî‚Äî
    if "customer_age" in df.columns:
        st.markdown("### Ph√¢n b·ªë ƒë·ªô tu·ªïi kh√°ch h√†ng")
        age_pdf = df.select("customer_age").toPandas().dropna()
        if not age_pdf.empty:
            fig10, ax10 = plt.subplots()
            ax10.hist(age_pdf["customer_age"], bins=20)
            ax10.set_title("Histogram ƒë·ªô tu·ªïi")
            ax10.set_xlabel("Tu·ªïi"); ax10.set_ylabel("T·∫ßn su·∫•t")
            st.pyplot(fig10)


# ==================================================
# üß© PH√ÇN T√çCH XU H∆Ø·ªöNG MUA S·∫ÆM (Trend Analysis)
# ==================================================
def run_trend_analysis(df):
    st.subheader("üìà Ph√¢n t√≠ch xu h∆∞·ªõng mua s·∫Øm")

    # Doanh thu theo th√°ng
    monthly_rev = (df.withColumn("month", date_format("purchase_ts", "yyyy-MM"))
                     .groupBy("month")
                     .agg(_sum("total_amount").alias("revenue"))
                     .orderBy("month"))

    pandas_monthly = monthly_rev.toPandas()
    if not pandas_monthly.empty:
        st.markdown("**Bi·ªÉu ƒë·ªì doanh thu theo th√°ng**")
        st.line_chart(pandas_monthly.set_index("month")["revenue"])
    else:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu doanh thu theo th√°ng.")

    # Top 5 danh m·ª•c s·∫£n ph·∫©m
    top_cat = (df.groupBy("product_category")
                 .agg(_sum("total_amount").alias("revenue"))
                 .orderBy(F.desc("revenue"))
                 .limit(5))

    pandas_cat = top_cat.toPandas()
    if not pandas_cat.empty:
        st.markdown("**Top 5 danh m·ª•c s·∫£n ph·∫©m c√≥ doanh thu cao nh·∫•t**")
        st.bar_chart(pandas_cat.set_index("product_category")["revenue"])

    # Ph∆∞∆°ng th·ª©c thanh to√°n ph·ªï bi·∫øn
    pay_method = (df.groupBy("payment_method")
                    .agg(count("*").alias("count"))
                    .orderBy(F.desc("count")))
    pandas_pay = pay_method.toPandas()
    if not pandas_pay.empty:
        st.markdown("**Ph√¢n b·ªë ph∆∞∆°ng th·ª©c thanh to√°n**")
        fig, ax = plt.subplots()
        ax.pie(
            pandas_pay["count"],
            labels=pandas_pay["payment_method"],
            autopct="%1.1f%%",
            startangle=140,
        )
        ax.axis("equal")
        st.pyplot(fig)


# ==================================================
# üß† PH√ÇN T√çCH H√ÄNH VI NG∆Ø·ªúI D√ôNG (Customer Behavior)
# ==================================================
def run_customer_behavior(df):
    st.subheader("üß† Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (RFM)")

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    rfm_df = (df.groupBy("customer_id")
                .agg(
                    F.max("purchase_ts").alias("last_purchase"),
                    count("*").alias("frequency"),
                    _sum("total_amount").alias("monetary")
                ))
    max_date = df.agg(F.max("purchase_ts")).collect()[0][0]
    rfm_df = rfm_df.withColumn(
        "recency_days", F.datediff(F.lit(max_date), F.col("last_purchase"))
    )

    pandas_rfm = rfm_df.toPandas()
    if pandas_rfm.empty:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu h√†nh vi ng∆∞·ªùi d√πng ƒë·ªÉ ph√¢n t√≠ch.")
        return

    # Bi·ªÉu ƒë·ªì ph√¢n t√°n R-F-M
    st.markdown("**Ph√¢n b·ªë RFM c·ªßa kh√°ch h√†ng**")
    fig, ax = plt.subplots(figsize=(8, 6))
    sc = ax.scatter(
        pandas_rfm["recency_days"],
        pandas_rfm["frequency"],
        c=pandas_rfm["monetary"],
        cmap="viridis",
        alpha=0.7,
    )
    plt.xlabel("Recency (days)")
    plt.ylabel("Frequency")
    plt.title("H√†nh vi mua s·∫Øm kh√°ch h√†ng (RFM)")
    plt.colorbar(sc, label="Monetary (T·ªïng chi ti√™u)")
    st.pyplot(fig)

    # B·∫£ng th·ªëng k√™ trung b√¨nh
    st.markdown("**Th·ªëng k√™ trung b√¨nh RFM**")
    st.dataframe(
        pandas_rfm[["recency_days", "frequency", "monetary"]]
        .describe()
        .T
        .rename(columns={"mean": "Gi√° tr·ªã trung b√¨nh"})
    )


# =========================
# ML: KMeans Customer Segmentation (RFM nh·∫π)
# =========================
def run_kmeans_segmentation(spark, parquet_path: str):
    from pyspark.ml.feature import VectorAssembler, StandardScaler
    from pyspark.ml.clustering import KMeans
    from pyspark.ml.evaluation import ClusteringEvaluator
    from pyspark.ml import Pipeline

    st.subheader("Ph√¢n c·ª•m kh√°ch h√†ng (KMeans, d·ª±a tr√™n RFM ƒë∆°n gi·∫£n)")

    # ƒê·∫£m b·∫£o Spark s·ªëng & ƒë·ªçc d·ªØ li·ªáu
    if not _spark_is_alive(spark):
        spark = ensure_spark(master)
    app_id = spark.sparkContext.applicationId
    try:
        df = load_parquet_cached(app_id, parquet_path)
    except Exception:
        df = spark.read.parquet(parquet_path)

    # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
    required = ["customer_id", "purchase_ts", "total_amount"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        st.error(f"Thi·∫øu c·ªôt cho RFM: {missing}")
        return

    # Lo·∫°i b·∫£n ghi thi·∫øu timestamp
    df_nonull = df.na.drop(subset=["purchase_ts"])
    if df_nonull.count() == 0:
        st.error("Kh√¥ng c√≥ 'purchase_ts' h·ª£p l·ªá. Ki·ªÉm tra ƒë·ªãnh d·∫°ng th·ªùi gian khi ETL.")
        return

    # T√≠nh RFM
    rfm = (df_nonull.groupBy("customer_id")
           .agg(_max("purchase_ts").alias("last_purchase"),
                count("*").alias("frequency"),
                _sum("total_amount").alias("monetary")))

    max_ts = df_nonull.agg(_max("purchase_ts").alias("mx")).collect()[0]["mx"]
    if max_ts is None:
        st.error("Kh√¥ng t√≠nh ƒë∆∞·ª£c m·ªëc th·ªùi gian l·ªõn nh·∫•t.")
        return

    rfm = rfm.withColumn("recency_days", expr(f"datediff(to_timestamp('{str(max_ts)}'), last_purchase)"))
    rfm_clean = rfm.na.drop(subset=["recency_days", "frequency", "monetary"])
    if rfm_clean.count() == 0:
        st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi l√†m s·∫°ch ƒë·ªÉ ph√¢n c·ª•m.")
        return

    # Pipeline ti·ªÅn x·ª≠ l√Ω
    features = ["recency_days", "frequency", "monetary"]
    assembler = VectorAssembler(inputCols=features, outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=False)

    # ---- T·ª∞ ƒê·ªòNG CH·ªåN k THEO SILHOUETTE ----
    evaluator = ClusteringEvaluator(featuresCol="features")
    ks = list(range(2, 9))  # d·∫£i k th·ª≠ (2..8). C√≥ th·ªÉ tƒÉng n·∫øu mu·ªën.
    scores = []

    # (TƒÉng t·ªëc nh·∫π v·ªõi sample n·∫øu qu√° l·ªõn)
    train_df = rfm_clean
    approx = rfm_clean.count()
    if approx > 200_000:
        train_df = rfm_clean.sample(False, 200_000 / approx, seed=42)

    best_model = None
    best_k = None
    best_score = float("-inf")

    for k in ks:
        model = Pipeline(stages=[assembler, scaler, KMeans(featuresCol="features", k=k, seed=42)]).fit(train_df)
        pred_k = model.transform(train_df)
        sil = evaluator.evaluate(pred_k)
        scores.append((k, sil))
        if sil > best_score:
            best_score, best_k, best_model = sil, k, model

    # Hi·ªÉn th·ªã b·∫£ng Silhouette theo k
    st.markdown("#### Ch·ªçn s·ªë c·ª•m t·ª± ƒë·ªông theo Silhouette")
    st.dataframe(
        pd.DataFrame(scores, columns=["k", "silhouette"]).sort_values("k"),
        use_container_width=True
    )
    st.success(f"S·ªë c·ª•m ƒë∆∞·ª£c ch·ªçn: **k = {best_k}**  (Silhouette = **{best_score:.4f}**)")

    # D√πng model t·ªët nh·∫•t ƒë·ªÉ g√°n c·ª•m cho TO√ÄN B·ªò d·ªØ li·ªáu rfm_clean
    res = best_model.transform(rfm_clean)

    # B·∫£ng k·∫øt qu·∫£ t·ª´ng kh√°ch
    st.dataframe(
        res.select("customer_id", "recency_days", "frequency", "monetary", "prediction")
           .orderBy("prediction", "customer_id")
           .limit(200)
           .toPandas(),
        use_container_width=True
    )

    # ---- B·∫¢NG PROFILE C·ª§M (b·∫°n n√≥i kh√¥ng th·∫•y, m√¨nh bu·ªôc hi·ªÉn th·ªã ·ªü ƒë√¢y) ----
    st.markdown("#### Profile t·ª´ng c·ª•m (trung b√¨nh R/F/M + s·ªë l∆∞·ª£ng)")
    prof = (res.groupBy("prediction")
              .agg(avg("recency_days").alias("avg_recency"),
                   avg("frequency").alias("avg_freq"),
                   avg("monetary").alias("avg_monetary"),
                   count("*").alias("n_customers"))
              .orderBy("prediction"))
    st.dataframe(prof.toPandas(), use_container_width=True)

    # (Tu·ª≥ ch·ªçn) Xu·∫•t k·∫øt qu·∫£
    with st.expander("Xu·∫•t k·∫øt qu·∫£ ph√¢n c·ª•m"):
        out_dir = st.text_input("ƒê∆∞·ªùng d·∫´n xu·∫•t (HDFS ho·∫∑c file://)", value=f"{parquet_path.rstrip('/')}_kmeans")
        if st.button("Ghi Parquet"):
            (res.select("customer_id", "recency_days", "frequency", "monetary", "prediction")
               .write.mode("overwrite").parquet(out_dir))
            st.success(f"ƒê√£ ghi: {out_dir}")

# =========================
# UI
# =========================
st.set_page_config(page_title="E-commerce Big Data App", layout="wide")
st.title("E-commerce Big Data App ‚Äî Spark ETL ‚Ä¢ Analytics ‚Ä¢ ML")

with st.sidebar:
    st.header("K·∫øt n·ªëi")
    master = st.text_input("Spark master", value=DEFAULT_SPARK_MASTER)
    log_level = st.selectbox("Spark log level", ["ERROR","WARN","INFO","DEBUG"], index=1)
    input_path = st.text_input("CSV HDFS path", value=DEFAULT_INPUT)
    parquet_out = st.text_input("Parquet output", value=PARQUET_DIR)
    ts_format = st.text_input("ƒê·ªãnh d·∫°ng th·ªùi gian (Purchase Date)", value="M/d/yyyy H:mm")
    # G·ª£i √Ω: d√πng 'yyyy-MM-dd HH:mm:ss' n·∫øu file c√≥ c·∫£ gi·ªù-ph√∫t-gi√¢y

    if st.button("Kh·ªüi t·∫°o Spark"):
        try:
            st.session_state["spark"] = start_spark(master=master, log_level=log_level)
            st.success("SparkSession ƒë√£ s·∫µn s√†ng.")
        except Exception as e:
            st.error(f"Kh·ªüi t·∫°o Spark th·∫•t b·∫°i: {e}")
            st.code(traceback.format_exc())

    if st.button("Restart SparkSession"):
        try:
            old = st.session_state.get("spark")
            if old is not None:
                try: old.stop()
                except Exception: pass
            _hard_reset_spark_jvm()
            # Xo√° cache resource g·∫Øn v·ªõi appId c≈© (n·∫øu c√≥)
            try: st.cache_resource.clear()
            except Exception: pass
            st.session_state["spark"] = start_spark(master=master, log_level=log_level)
            st.success("ƒê√£ restart SparkSession.")
        except Exception as e:
            st.error(f"Restart Spark th·∫•t b·∫°i: {e}")
            st.code(traceback.format_exc())


spark = st.session_state.get("spark")
if spark is None:
    st.warning("H√£y kh·ªüi t·∫°o Spark ·ªü thanh b√™n.")
    st.stop()

st.markdown("### 1) ETL")
if st.button("Ch·∫°y ETL t·ª´ CSV ‚Üí Parquet"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        out = run_etl(spark, src_path=input_path, dst_parquet=parquet_out, ts_format=ts_format.strip())
        st.session_state["parquet_path"] = out
    except Exception as e:
        st.error(f"L·ªói ETL: {e}")
        st.code(traceback.format_exc())

st.markdown("### 2) Analytics")
ppath = st.text_input("Parquet ƒë√£ ETL", value=st.session_state.get("parquet_path", PARQUET_DIR))
if st.button("Ch·∫°y Analytics"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        run_analytics(spark, ppath)
    except Exception as e:
        st.error(f"L·ªói Analytics: {e}")
        st.code(traceback.format_exc())

st.markdown("### 3) Ph√¢n t√≠ch n√¢ng cao")
    if st.button("üìä Ph√¢n t√≠ch xu h∆∞·ªõng mua s·∫Øm"):
        try:
            spark = start_spark(log_level=log_level)
            df = spark.read.parquet(ppath)
            run_trend_analysis(df)
        except Exception as e:
            st.error(f"L·ªói khi ph√¢n t√≠ch xu h∆∞·ªõng: {e}")
            st.code(traceback.format_exc())

    if st.button("üß† Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng"):
        try:
            spark = start_spark(log_level=log_level)
            df = spark.read.parquet(ppath)
            run_customer_behavior(df)
        except Exception as e:
            st.error(f"L·ªói khi ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng: {e}")
            st.code(traceback.format_exc())

st.markdown("### 4) Machine Learning")

if st.button("Ph√¢n c·ª•m kh√°ch h√†ng (KMeans)"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        run_kmeans_segmentation(spark, ppath)
    except Exception as e:
        st.error(f"L·ªói ML (KMeans): {e}")
        st.code(traceback.format_exc())
