import os
import traceback
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
import numpy as np
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.clustering import KMeans
from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, DoubleType
)
from pyspark.sql.functions import (
    col, to_timestamp, date_format, coalesce, trim, regexp_replace,
    countDistinct, count, sum as _sum, avg, expr, when, max as _max
)



# =========================
# Config m·∫∑c ƒë·ªãnh
# =========================
DEFAULT_SPARK_MASTER = os.getenv("SPARK_MASTER_URL", os.getenv("SPARK_MASTER", "spark://spark-master:7077"))
DEFAULT_INPUT = "hdfs://namenode:8020/input/ecommerce.csv"
DEFAULT_WAREHOUSE = os.getenv("WAREHOUSE_PATH", "hdfs://namenode:8020/warehouse")
DEFAULT_OUTPUT = os.getenv("OUTPUT_PATH", "hdfs://namenode:8020/output")
PARQUET_DIR = f"{DEFAULT_WAREHOUSE.rstrip('/')}/ecommerce_parquet"

# =========================
# Spark helpers (1 version only)
# =========================
# ---- CACHE: ƒë·ªçc Parquet 1 l·∫ßn cho c·∫£ Analytics & ML ----
def add_churn_from_recency(df, customer_col="customer_id", ts_col="purchase_ts", cutoff_days=180):
    """
    T·∫°o c·ªôt churn d·ª±a tr√™n 'recency':
      - T√¨m l·∫ßn mua g·∫ßn nh·∫•t c·ªßa t·ª´ng kh√°ch h√†ng.
      - N·∫øu s·ªë ng√†y t·ª´ l·∫ßn mua g·∫ßn nh·∫•t t·ªõi m·ªëc th·ªùi gian max trong dataset > cutoff_days => churn=1, ng∆∞·ª£c l·∫°i 0.
    Tr·∫£ v·ªÅ DataFrame ƒë√£ c√≥ c·ªôt 'churn' (int).
    """
    mx = df.agg(_max(ts_col).alias("mx")).collect()[0]["mx"]
    if mx is None:
        # Kh√¥ng c√≥ timestamp h·ª£p l·ªá -> kh√¥ng th·ªÉ t√≠nh recency: set churn=0 n·∫øu ch∆∞a c√≥
        return df.withColumn("churn", when(col("churn").isNull(), 0).otherwise(col("churn")).cast("int"))

    last_by_cust = (
        df.groupBy(customer_col)
          .agg(_max(ts_col).alias("last_purchase"))
          .withColumn("days_since_last", expr(f"datediff(to_timestamp('{str(mx)}'), last_purchase)"))
          .withColumn("churn_calc", (col("days_since_last") > cutoff_days).cast("int"))
          .select(customer_col, "churn_calc")
    )

    df2 = (
        df.join(last_by_cust, on=customer_col, how="left")
          .withColumn("churn", coalesce(col("churn").cast("int"), col("churn_calc")).cast("int"))
          .drop("churn_calc")
    )
    return df2
@st.cache_resource(show_spinner=False)
def load_parquet_cached(app_id: str, parquet_path: str):
    """
    Cache DataFrame theo app_id hi·ªán t·∫°i ƒë·ªÉ tr√°nh gi·ªØ DF g·∫Øn v·ªõi SparkSession ƒë√£ stop.
    """
    spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()
    df = spark.read.parquet(parquet_path).cache()   # cache tr√™n executors
    _ = df.count()                                   # materialize ƒë·ªÉ l·∫ßn sau r·∫•t nhanh
    return df


def _spark_is_alive(spark):
    try:
        return spark is not None and not spark.sparkContext._jsc.sc().isStopped()
    except Exception:
        return False

def _hard_reset_spark_jvm():
    """Xo√° session m·∫∑c ƒë·ªãnh/active v√† stop SparkContext c√≤n treo trong JVM."""
    try:
        # clear default/active SparkSession (t·∫ßng JVM)
        SparkSession.clearActiveSession()
        SparkSession.clearDefaultSession()
    except Exception:
        pass
    try:
        # stop SparkContext ƒëang active n·∫øu c√≤n
        from pyspark import SparkContext
        sc = SparkContext._active_spark_context
        if sc is not None:
            sc.stop()
    except Exception:
        pass

def start_spark(app_name: str = "EcomBigDataApp", master: str = DEFAULT_SPARK_MASTER, log_level: str = "WARN"):
    # L·∫ßn 1: th·ª≠ t·∫°o b√¨nh th∆∞·ªùng
    try:
        spark = (SparkSession.builder
                 .appName(app_name)
                 .master(master)
                 .config("spark.sql.shuffle.partitions", "64")
                 .config("spark.ui.showConsoleProgress", "true")
                 .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                 .getOrCreate())
        if not _spark_is_alive(spark):
            raise RuntimeError("SparkContext stopped right after getOrCreate()")
        spark.sparkContext.setLogLevel(log_level)
        return spark
    except Exception as e1:
        print("[WARN] First attempt failed ‚Üí reset JVM sessions then retry. Reason:", repr(e1))
        # D·ªåN S·∫†CH r·ªìi th·ª≠ l·∫°i v·ªõi master th·∫≠t
        _hard_reset_spark_jvm()
        try:
            spark = (SparkSession.builder
                     .appName(app_name)
                     .master(master)
                     .config("spark.sql.shuffle.partitions", "64")
                     .config("spark.ui.showConsoleProgress", "true")
                     .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                     .getOrCreate())
            if not _spark_is_alive(spark):
                raise RuntimeError("SparkContext stopped again after getOrCreate()")
            spark.sparkContext.setLogLevel(log_level)
            return spark
        except Exception as e2:
            print("[WARN] Second attempt with master failed ‚Üí fallback local[*]. Reason:", repr(e2))
            _hard_reset_spark_jvm()
            spark = (SparkSession.builder
                     .appName(app_name + "-local")
                     .master("local[*]")
                     .config("spark.sql.shuffle.partitions", "8")
                     .config("spark.ui.showConsoleProgress", "true")
                     .config("spark.sql.legacy.timeParserPolicy", "LEGACY")
                     .getOrCreate())
            spark.sparkContext.setLogLevel(log_level)
            return spark
def ensure_spark(master: str, app_name: str = "EcomBigDataApp", log_level: str = "WARN"):
    spark = st.session_state.get("spark")
    if not _spark_is_alive(spark):
        _hard_reset_spark_jvm()
        spark = start_spark(app_name=app_name, master=master, log_level=log_level)
        st.session_state["spark"] = spark
    return spark


# =========================
# Schema dataset theo m√¥ t·∫£
# =========================
SCHEMA = StructType([
    StructField("Customer ID", StringType(), True),
    StructField("Customer Name", StringType(), True),
    StructField("Customer Age", IntegerType(), True),
    StructField("Gender", StringType(), True),
    StructField("Purchase Date", StringType(), True),          # parse sau
    StructField("Product Category", StringType(), True),
    StructField("Product Price", DoubleType(), True),
    StructField("Quantity", IntegerType(), True),
    StructField("Total Purchase Amount", DoubleType(), True),
    StructField("Payment Method", StringType(), True),
    StructField("Returns", IntegerType(), True),               # 0/1
    StructField("Churn", IntegerType(), True)                  # 0/1
])

# =========================
# ETL
# =========================
def run_etl(spark, src_path: str, dst_parquet: str, ts_format: str):
    st.info(f"ƒê·ªçc CSV t·ª´: {src_path}")

    # ƒê·ªçc theo schema ƒë·ªãnh nghƒ©a s·∫µn (SCHEMA / SCHEMA_WITH_CUSTOMER)
    df = (
        spark.read
        .option("header", "true")
        .option("inferSchema", "true")  
        .csv(src_path)
    )

    # --- ƒê·ªïi t√™n c·ªôt v·ªÅ schema ng·∫Øn g·ªçn/th·ªëng nh·∫•t ---
    rename_pairs = {
        "Customer ID": "customer_id",
        "Customer": "customer_id",                  # h·ªó tr·ª£ header "Customer"
        "Customer Name": "customer_name",
        "Customer Age": "customer_age",
        "Gender": "gender",
        "Purchase Date": "purchase_date_raw",
        "Product Category": "product_category",
        "Product Price": "product_price",
        "Quantity": "quantity",
        "Total Purchase Amount": "total_amount",
        "Payment Method": "payment_method",
        "Returns": "returns",
        "Churn": "churn",
    }
    for old, new in rename_pairs.items():
        if old in df.columns and new not in df.columns:
            df = df.withColumnRenamed(old, new)

    # --- L√†m s·∫°ch s·ªë & √©p ki·ªÉu an to√†n ---
    for num_col in ["product_price", "total_amount"]:
        if num_col in df.columns:
            df = df.withColumn(
                num_col,
                regexp_replace(col(num_col).cast("string"), r"[^0-9\.\-]", "").cast("double")
            )
    if "quantity" in df.columns:
        df = df.withColumn("quantity",
            regexp_replace(col("quantity").cast("string"), r"[^0-9\-]", "").cast("int")
        )
    if "returns" in df.columns:
        df = df.withColumn("returns", col("returns").cast("int"))
    if "churn" in df.columns:
        df = df.withColumn("churn", col("churn").cast("int"))

    # --- Parse th·ªùi gian ƒëa ƒë·ªãnh d·∫°ng ---
    raw = "purchase_date_raw"
    if raw in df.columns:
        df = df.withColumn(raw, trim(col(raw)))
        df = df.withColumn(raw, regexp_replace(col(raw), "T", " "))
        df = df.withColumn(raw, regexp_replace(col(raw), "Z$", ""))
        df = df.withColumn(raw, regexp_replace(col(raw), r"[\u00A0\u2007\u202F]", " "))
        df = df.withColumn(raw, regexp_replace(col(raw), r"\s+", " "))

        patterns = []
        if ts_format:
            patterns.append(ts_format)

        # ∆Øu ti√™n d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n: "9/8/2020 9:38" (kh√¥ng gi√¢y, 1 ch·ªØ s·ªë)
        patterns += [
            "M/d/yyyy H:mm",
            "M/d/yyyy h:mm a",
            "MM/dd/yyyy HH:mm",
            "dd/M/yyyy H:mm",
            "dd/MM/yyyy HH:mm",
            "M/d/yyyy H:mm:ss",
            "MM/dd/yyyy HH:mm:ss",
            "dd/M/yyyy H:mm:ss",
            "dd/MM/yyyy HH:mm:ss",
            "yyyy-MM-dd",
            "MM/dd/yyyy",
            "dd/MM/yyyy",
        ]

        ts_exprs = [to_timestamp(col(raw), p) for p in patterns]
        df = df.withColumn("purchase_ts", coalesce(*ts_exprs))
        df = df.withColumn("purchase_date", date_format(col("purchase_ts"), "yyyy-MM-dd"))

        # C·∫£nh b√°o n·∫øu g·∫ßn nh∆∞ kh√¥ng parse ƒë∆∞·ª£c
        null_ratio = df.select((col("purchase_ts").isNull()).cast("int").alias("isnull")) \
                       .agg({"isnull": "avg"}).collect()[0][0]
        if null_ratio and null_ratio >= 0.9999:
            st.warning(
                "Kh√¥ng parse ƒë∆∞·ª£c c·ªôt th·ªùi gian 'Purchase Date'. "
                "H√£y nh·∫≠p ƒë√∫ng ƒë·ªãnh d·∫°ng ·ªü √¥ 'ƒê·ªãnh d·∫°ng th·ªùi gian' (v√≠ d·ª•: M/d/yyyy H:mm). "
                "App v·∫´n ghi Parquet nh∆∞ng c√°c bi·ªÉu ƒë·ªì theo ng√†y s·∫Ω r·ªóng."
            )
    else:
        df = df.withColumn("purchase_ts", None).withColumn("purchase_date", None)

    # --- B·ªï khuy·∫øt churn theo recency n·∫øu thi·∫øu ho·∫∑c ph·∫ßn l·ªõn NULL ---
    needs_fill = True
    if "churn" in df.columns:
        null_ratio = df.select((col("churn").isNull()).cast("int").alias("isnull")) \
                       .agg({"isnull": "avg"}).collect()[0][0]
        needs_fill = (null_ratio is None) or (null_ratio > 0.5)

    cutoff_days = 180  # c√≥ th·ªÉ ƒë∆∞a ra sidebar n·∫øu mu·ªën ƒëi·ªÅu ch·ªânh ƒë·ªông
    if needs_fill:
        df = add_churn_from_recency(df, customer_col="customer_id",
                                    ts_col="purchase_ts", cutoff_days=cutoff_days)

    # --- ƒêi·ªÅn NA cho m·ªôt s·ªë c·ªôt ph√¢n lo·∫°i ---
    for c in ["gender", "product_category", "payment_method"]:
        if c in df.columns:
            df = df.fillna({c: "unknown"})

    # --- Ghi Parquet (overwrite), partition theo ng√†y n·∫øu c√≥ ---
    writer = df.write.mode("overwrite")
    if "purchase_date" in df.columns:
        writer = writer.partitionBy("purchase_date")
    writer.parquet(dst_parquet)

    st.success(f"ETL ho√†n t·∫•t ‚Üí {dst_parquet}")
    return dst_parquet

# =========================
# Analytics
# =========================
def run_analytics(spark, parquet_path: str):
    st.subheader("Analytics")
    if not _spark_is_alive(spark):
        sparkContext = ensure_spark(master)  # ho·∫∑c raise l·ªói tu·ª≥ b·∫°n ƒë√£ vi·∫øt
        spark = ensure_spark(master)
    app_id = spark.sparkContext.applicationId
    df = load_parquet_cached(app_id, parquet_path)

    # T·ªïng quan
    total_rows = df.count()
    n_cols = len(df.columns)
    st.write(f"**S·ªë d√≤ng:** {total_rows:,} | **S·ªë c·ªôt:** {n_cols}")
    with st.expander("Danh s√°ch c·ªôt"):
        st.code(", ".join(df.columns))

    # ‚Äî‚Äî KPI ‚Äî‚Äî
    st.markdown("### KPIs t·ªïng quan")
    kpi = {}
    if "customer_id" in df.columns:
        kpi["S·ªë kh√°ch h√†ng"] = df.select(countDistinct("customer_id").alias("n")).collect()[0]["n"]
    if "product_category" in df.columns:
        kpi["S·ªë category"] = df.select(countDistinct("product_category").alias("n")).collect()[0]["n"]
    if "total_amount" in df.columns:
        kpi["T·ªïng doanh thu"] = df.agg(_sum("total_amount").alias("rev")).collect()[0]["rev"]
    if "returns" in df.columns:
        kpi["T·ªâ l·ªá tr·∫£ h√†ng (%)"] = round((df.agg(avg(col("returns").cast("double")).alias("r"))
                                             .collect()[0]["r"] or 0) * 100, 2)
    if "churn" in df.columns:
        kpi["T·ªâ l·ªá churn (%)"] = round((df.agg(avg(col("churn").cast("double")).alias("c"))
                                         .collect()[0]["c"] or 0) * 100, 2)

    if kpi:
        for k, v in kpi.items():
            st.write(f"- **{k}:** {v}")
    else:
        st.write("Kh√¥ng ƒë·ªß c·ªôt ƒë·ªÉ t√≠nh KPI.")

    # ‚Äî‚Äî Doanh thu theo ng√†y (Bi·ªÉu ƒë·ªì t∆∞∆°ng t√°c c·ªßa Streamlit) ‚Äî‚Äî
    if "purchase_date" in df.columns and "total_amount" in df.columns:
        st.markdown("### Doanh thu & ƒê∆°n h√†ng theo ng√†y")

        daily = (
            df.groupBy("purchase_date")
              .agg(_sum("total_amount").alias("revenue"),
                   count("*").alias("orders"))
              .orderBy("purchase_date")
        )
        pdf = daily.toPandas()

        if pdf.empty or pdf["purchase_date"].isna().all():
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ng√†y (c√≥ th·ªÉ do kh√¥ng parse ƒë∆∞·ª£c 'Purchase Date').")
        else:
            # Chu·∫©n ho√° th·ªùi gian & sort
            pdf["purchase_date"] = pd.to_datetime(pdf["purchase_date"], errors="coerce")
            pdf = pdf.dropna(subset=["purchase_date"]).sort_values("purchase_date")
            
            if pdf.empty:
                st.warning("Kh√¥ng c√≥ b·∫£n ghi h·ª£p l·ªá sau khi chu·∫©n ho√° th·ªùi gian.")
            else:
                # T√≠nh ƒë∆∞·ªùng trung b√¨nh tr∆∞·ª£t cho Revenue
                pdf["rev_ma7"] = pdf["revenue"].rolling(7, min_periods=1).mean()
                
                # ƒê·∫∑t 'purchase_date' l√†m index ƒë·ªÉ Streamlit v·∫Ω bi·ªÉu ƒë·ªì
                pdf_indexed = pdf.set_index("purchase_date")

                # --- Bi·ªÉu ƒë·ªì 1: Doanh thu (Revenue) ---
                st.markdown("#### Xu h∆∞·ªõng doanh thu (Revenue & MA-7)")
                st.write("Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán doanh thu h√†ng ng√†y v√† ƒë∆∞·ªùng trung b√¨nh tr∆∞·ª£t 7 ng√†y (MA-7).")
                # st.line_chart s·∫Ω t·ª± ƒë·ªông v·∫Ω 2 c·ªôt 'revenue' v√† 'rev_ma7'
                st.line_chart(pdf_indexed[["revenue", "rev_ma7"]])

                # --- Bi·ªÉu ƒë·ªì 2: S·ªë l∆∞·ª£ng ƒë∆°n h√†ng (Orders) ---
                st.markdown("#### S·ªë l∆∞·ª£ng ƒë∆°n h√†ng h√†ng ng√†y")
                st.write("Bi·ªÉu ƒë·ªì c·ªôt th·ªÉ hi·ªán t·ªïng s·ªë ƒë∆°n h√†ng m·ªói ng√†y.")
                st.bar_chart(pdf_indexed[["orders"]])


    # ‚Äî‚Äî Top Category theo s·ªë ƒë∆°n & doanh thu ‚Äî‚Äî
    if "product_category" in df.columns:
        st.markdown("### Top Category")
        top_n = 10
        top_cat = (df.groupBy("product_category")
                     .agg(count("*").alias("cnt"), _sum("total_amount").alias("revenue"))
                     .orderBy(col("cnt").desc())
                     .limit(top_n))
        pdf = top_cat.toPandas()
        st.dataframe(pdf, use_container_width=True)

        if not pdf.empty:
            # theo s·ªë ƒë∆°n (barh, d·ªÖ ƒë·ªçc label d√†i)
            pdf_cnt = pdf.sort_values("cnt", ascending=True)
            fig4, ax4 = plt.subplots()
            ax4.barh(pdf_cnt["product_category"].astype(str), pdf_cnt["cnt"])
            ax4.set_title(f"Top {top_n} Category (theo s·ªë ƒë∆°n)")
            ax4.set_xlabel("Count"); ax4.set_ylabel("Category")
            st.pyplot(fig4)

            # theo doanh thu (barh)
            pdf_rev = pdf.sort_values("revenue", ascending=True)
            fig5, ax5 = plt.subplots()
            ax5.barh(pdf_rev["product_category"].astype(str), pdf_rev["revenue"])
            ax5.set_title(f"Top {top_n} Category (theo doanh thu)")
            ax5.set_xlabel("Revenue"); ax5.set_ylabel("Category")
            st.pyplot(fig5)


    # ‚Äî‚Äî Ph∆∞∆°ng th·ª©c thanh to√°n ‚Äî‚Äî
    if "payment_method" in df.columns:
        st.markdown("### Ph√¢n b·ªë ph∆∞∆°ng th·ª©c thanh to√°n")
        pm = df.groupBy("payment_method").agg(count("*").alias("cnt"),
                                              _sum("total_amount").alias("revenue")) \
               .orderBy(col("cnt").desc())
        pdf = pm.toPandas()
        st.dataframe(pdf, use_container_width=True)

        if not pdf.empty:
            fig6, ax6 = plt.subplots()
            ax6.bar(pdf["payment_method"].astype(str), pdf["cnt"])
            ax6.set_title("S·ªë ƒë∆°n theo ph∆∞∆°ng th·ª©c thanh to√°n")
            ax6.set_xlabel("Payment Method"); ax6.set_ylabel("Count")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig6)

            fig7, ax7 = plt.subplots()
            ax7.bar(pdf["payment_method"].astype(str), pdf["revenue"])
            ax7.set_title("Doanh thu theo ph∆∞∆°ng th·ª©c thanh to√°n")
            ax7.set_xlabel("Payment Method"); ax7.set_ylabel("Revenue")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig7)

    # ‚Äî‚Äî Returns & Churn theo ng√†y (n·∫øu c√≥ c·ªôt) ‚Äî‚Äî
    if "purchase_date" in df.columns and "returns" in df.columns:
        st.markdown("### T·ªâ l·ªá tr·∫£ h√†ng theo ng√†y")
        rate = (df.groupBy("purchase_date")
                  .agg(avg(col("returns").cast("double")).alias("return_rate"))
                  .orderBy("purchase_date")).toPandas()
        
        if not rate.empty and rate["purchase_date"].notna().any():
            # Chu·∫©n ho√° th·ªùi gian & set index
            rate["purchase_date"] = pd.to_datetime(rate["purchase_date"], errors="coerce")
            rate = rate.dropna(subset=["purchase_date"]).set_index("purchase_date")
            
            if not rate.empty:
                st.write("Bi·ªÉu ƒë·ªì ƒë∆∞·ªùng th·ªÉ hi·ªán t·ªâ l·ªá tr·∫£ h√†ng (return rate) theo th·ªùi gian.")
                st.line_chart(rate[["return_rate"]])
            else:
                st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu t·ªâ l·ªá tr·∫£ h√†ng h·ª£p l·ªá sau khi chu·∫©n ho√° th·ªùi gian.")
        else:
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì t·ªâ l·ªá tr·∫£ h√†ng.")

    if "churn" in df.columns and "gender" in df.columns:
        st.markdown("### T·ªâ l·ªá churn theo gi·ªõi t√≠nh")
        churn_g = (df.groupBy("gender")
                     .agg(avg(col("churn").cast("double")).alias("churn_rate"))
                     .orderBy("gender")).toPandas()
        if not churn_g.empty:
            fig9, ax9 = plt.subplots()
            ax9.bar(churn_g["gender"].astype(str), churn_g["churn_rate"])
            ax9.set_title("Churn rate theo gi·ªõi t√≠nh")
            ax9.set_xlabel("Gender"); ax9.set_ylabel("Churn rate")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig9)

    # ‚Äî‚Äî Ph√¢n b·ªë tu·ªïi (n·∫øu c√≥) ‚Äî‚Äî
    if "customer_age" in df.columns:
        st.markdown("### Ph√¢n b·ªë ƒë·ªô tu·ªïi kh√°ch h√†ng")
        age_pdf = df.select("customer_age").toPandas().dropna()
        if not age_pdf.empty:
            data = age_pdf["customer_age"].astype(float)
            # L·ªçc ra c√°c gi√° tr·ªã tu·ªïi h·ª£p l·ªá (v√≠ d·ª•: 0-100) ƒë·ªÉ tr√°nh l√†m h·ªèng bi·ªÉu ƒë·ªì
            data = data[(data > 0) & (data < 100)] 
            
            if len(data) > 0:
                # T√≠nh s·ªë bins (nh√≥m)
                bins_count = int(np.clip(np.sqrt(len(data)), 10, 40)) # sqrt(n) trong [10..40]
                
                # T√≠nh to√°n gi√° tr·ªã histogram (t·∫ßn su·∫•t v√† c√°c c·∫°nh c·ªßa bin)
                hist, bin_edges = np.histogram(data, bins=bins_count)
                
                # T·∫°o DataFrame cho st.bar_chart
                # L·∫•y nh√£n l√† ƒëi·ªÉm gi·ªØa c·ªßa m·ªói bin ƒë·ªÉ bi·ªÉu ƒë·ªì hi·ªÉn th·ªã ƒë√∫ng
                bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
                hist_df = pd.DataFrame({
                    'age_center': bin_centers, # Tr·ª•c X: Tu·ªïi (trung t√¢m nh√≥m)
                    'count': hist              # Tr·ª•c Y: S·ªë l∆∞·ª£ng kh√°ch h√†ng
                }).set_index('age_center')
                
                st.write(f"Bi·ªÉu ƒë·ªì c·ªôt th·ªÉ hi·ªán t·∫ßn su·∫•t kh√°ch h√†ng theo c√°c nh√≥m tu·ªïi (chia th√†nh {bins_count} nh√≥m).")
                st.bar_chart(hist_df[['count']])
            else:
                st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu tu·ªïi h·ª£p l·ªá (0-100) ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë.")
        else:
            st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu tu·ªïi ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë.")


# ==================================================
# üß© PH√ÇN T√çCH XU H∆Ø·ªöNG MUA S·∫ÆM (Trend Analysis)
# ==================================================
def run_trend_analysis(df):
    st.subheader("üìà Ph√¢n t√≠ch xu h∆∞·ªõng mua s·∫Øm")

        # Doanh thu theo th√°ng
    monthly_rev = (df.withColumn("month", date_format("purchase_ts", "yyyy-MM"))
                     .groupBy("month")
                     .agg(_sum("total_amount").alias("revenue"))
                     .orderBy("month"))

    pandas_monthly = monthly_rev.toPandas()

    if not pandas_monthly.empty:
        import matplotlib.dates as mdates

        # Chu·∫©n ho√° m·ªëc th√°ng, ƒëi·ªÅn th√°ng tr·ªëng = 0
        pandas_monthly["month"] = pd.to_datetime(pandas_monthly["month"], format="%Y-%m", errors="coerce")
        pandas_monthly = pandas_monthly.dropna(subset=["month"]).sort_values("month")

        if not pandas_monthly.empty:
            # Reindex theo d·∫£i th√°ng ƒë·∫ßy ƒë·ªß
            full_range = pd.period_range(
                pandas_monthly["month"].min().to_period("M"),
                pandas_monthly["month"].max().to_period("M"),
                freq="M"
            ).to_timestamp()
            pm = pandas_monthly.set_index("month").reindex(full_range, fill_value=0).rename_axis("month").reset_index()

            fig, ax = plt.subplots()
            ax.plot(pm["month"], pm["revenue"], linewidth=2)
            locator = mdates.AutoDateLocator(minticks=4, maxticks=12)
            formatter = mdates.ConciseDateFormatter(locator)
            ax.xaxis.set_major_locator(locator)
            ax.xaxis.set_major_formatter(formatter)
            ax.set_title("Doanh thu theo th√°ng")
            ax.set_xlabel("Th√°ng"); ax.set_ylabel("Revenue")
            st.pyplot(fig)
    else:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu doanh thu theo th√°ng.")

    # Top 5 danh m·ª•c s·∫£n ph·∫©m
    top_cat = (df.groupBy("product_category")
                 .agg(_sum("total_amount").alias("revenue"))
                 .orderBy(F.desc("revenue"))
                 .limit(5))

    pandas_cat = top_cat.toPandas()
    if not pandas_cat.empty:
        st.markdown("**Top 5 danh m·ª•c s·∫£n ph·∫©m c√≥ doanh thu cao nh·∫•t**")
        st.bar_chart(pandas_cat.set_index("product_category")["revenue"])

        # Ph∆∞∆°ng th·ª©c thanh to√°n ph·ªï bi·∫øn (bar chart thay v√¨ pie)
    pay_method = (df.groupBy("payment_method")
                    .agg(count("*").alias("count"))
                    .orderBy(F.desc("count")))
    pandas_pay = pay_method.toPandas()
    if not pandas_pay.empty:
        st.markdown("**Ph√¢n b·ªë ph∆∞∆°ng th·ª©c thanh to√°n**")
        fig, ax = plt.subplots()
        ax.bar(pandas_pay["payment_method"].astype(str), pandas_pay["count"])
        ax.set_title("S·ªë ƒë∆°n theo ph∆∞∆°ng th·ª©c thanh to√°n")
        ax.set_xlabel("Payment Method"); ax.set_ylabel("Count")
        plt.xticks(rotation=45, ha='right')
        st.pyplot(fig)



# ==================================================
# üß† PH√ÇN T√çCH H√ÄNH VI NG∆Ø·ªúI D√ôNG (Customer Behavior)
# ==================================================
def run_customer_behavior(df):
    st.subheader("üß† Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (RFM)")

    # Chu·∫©n b·ªã d·ªØ li·ªáu
    rfm_df = (df.groupBy("customer_id")
                .agg(
                    F.max("purchase_ts").alias("last_purchase"),
                    count("*").alias("frequency"),
                    _sum("total_amount").alias("monetary")
                ))
    
    max_date_row = df.agg(F.max("purchase_ts")).collect()
    if not max_date_row or max_date_row[0][0] is None:
        st.error("Kh√¥ng th·ªÉ t√≠nh to√°n RFM do thi·∫øu m·ªëc th·ªùi gian (max_date). Ki·ªÉm tra l·∫°i d·ªØ li·ªáu 'Purchase Date'.")
        return
    max_date = max_date_row[0][0]
        
    rfm_df = rfm_df.withColumn(
        "recency_days", F.datediff(F.lit(max_date), F.col("last_purchase"))
    )

    rfm_df = rfm_df.select("customer_id", "recency_days", "frequency", "monetary")
    pandas_rfm = rfm_df.toPandas()
    if pandas_rfm.empty:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu h√†nh vi ng∆∞·ªùi d√πng ƒë·ªÉ ph√¢n t√≠ch.")
        return

    # Bi·ªÉu ƒë·ªì ph√¢n t√°n R-F-M
    st.markdown("**Ph√¢n b·ªë RFM c·ªßa kh√°ch h√†ng (T·ªïng quan)**")
    fig, ax = plt.subplots(figsize=(8, 6))
    sc = ax.scatter(
        pandas_rfm["recency_days"],
        pandas_rfm["frequency"],
        c=pandas_rfm["monetary"],
        cmap="viridis",
        alpha=0.7,
    )
    plt.xlabel("Recency (days)")
    plt.ylabel("Frequency")
    plt.title("H√†nh vi mua s·∫Øm kh√°ch h√†ng (R vs F, m√†u l√† M)")
    plt.colorbar(sc, label="Monetary (T·ªïng chi ti√™u)")
    st.pyplot(fig)

    # --- Th√™m bi·ªÉu ƒë·ªì ph√¢n b·ªë chi ti·∫øt ---
    st.markdown("**Ph√¢n b·ªë chi ti·∫øt c·ªßa Recency, Frequency, v√† Monetary**")
    
    # Chia layout th√†nh 3 c·ªôt ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("#### Recency (S·ªë ng√†y t·ª´ l·∫ßn mua cu·ªëi)")
        fig_r, ax_r = plt.subplots()
        ax_r.hist(pandas_rfm['recency_days'].dropna(), bins=30, color='skyblue', edgecolor='black')
        ax_r.set_title("Ph√¢n b·ªë Recency")
        ax_r.set_xlabel("S·ªë ng√†y")
        ax_r.set_ylabel("S·ªë l∆∞·ª£ng kh√°ch h√†ng")
        st.pyplot(fig_r)

    with col2:
        st.markdown("#### Frequency (T·ªïng s·ªë l·∫ßn mua)")
        # L·∫•y d·ªØ li·ªáu F, l·ªçc c√°c gi√° tr·ªã ngo·∫°i l·ªá (v√≠ d·ª•: > 99th percentile) ƒë·ªÉ bi·ªÉu ƒë·ªì d·ªÖ nh√¨n h∆°n
        f_data = pandas_rfm['frequency'].dropna()
        if not f_data.empty:
            f_q99 = f_data.quantile(0.99)
            f_data_clipped = f_data[f_data <= f_q99]
            
            fig_f, ax_f = plt.subplots()
            ax_f.hist(f_data_clipped, bins=30, color='lightgreen', edgecolor='black')
            ax_f.set_title(f"Ph√¢n b·ªë Frequency (l·ªçc gi√° tr·ªã > {f_q99:.0f})")
            ax_f.set_xlabel("S·ªë l·∫ßn mua")
            ax_f.set_ylabel("S·ªë l∆∞·ª£ng kh√°ch h√†ng")
            st.pyplot(fig_f)
        else:
            st.write("Kh√¥ng c√≥ d·ªØ li·ªáu Frequency.")

    with col3:
        st.markdown("#### Monetary (T·ªïng chi ti√™u)")
        # L·ªçc gi√° tr·ªã ngo·∫°i l·ªá t∆∞∆°ng t·ª± Frequency
        m_data = pandas_rfm['monetary'].dropna()
        if not m_data.empty:
            m_q99 = m_data.quantile(0.99)
            m_data_clipped = m_data[m_data <= m_q99]
            
            fig_m, ax_m = plt.subplots()
            ax_m.hist(m_data_clipped, bins=30, color='salmon', edgecolor='black')
            ax_m.set_title(f"Ph√¢n b·ªë Monetary (l·ªçc gi√° tr·ªã > {m_q99:,.0f})")
            ax_m.set_xlabel("T·ªïng chi ti√™u")
            ax_m.set_ylabel("S·ªë l∆∞·ª£ng kh√°ch h√†ng")
            st.pyplot(fig_m)
        else:
            st.write("Kh√¥ng c√≥ d·ªØ li·ªáu Monetary.")

    # B·∫£ng th·ªëng k√™ trung b√¨nh
    st.markdown("**Th·ªëng k√™ trung b√¨nh RFM**")
    st.dataframe(
        pandas_rfm[["recency_days", "frequency", "monetary"]]
        .describe()
        .T
        .rename(columns={"mean": "Gi√° tr·ªã trung b√¨nh"})
    )


# =========================
# ML: KMeans Customer Segmentation (RFM nh·∫π)
# =========================
def _interpret_cluster(r, f, m, g_r, g_f, g_m):
    """H√†m tr·ª£ gi√∫p ƒë·ªÉ t·ª± ƒë·ªông di·ªÖn gi·∫£i √Ω nghƒ©a c·ª•m."""
    
    # So s√°nh R, F, M c·ªßa c·ª•m v·ªõi trung b√¨nh chung
    # (T·ªët/X·∫•u/TB)
    r_score = "Th·∫•p (T·ªët)" if r < g_r * 0.9 else "Cao (X·∫•u)" if r > g_r * 1.1 else "Trung b√¨nh"
    f_score = "Cao (T·ªët)" if f > g_f * 1.1 else "Th·∫•p (X·∫•u)" if f < g_f * 0.9 else "Trung b√¨nh"
    m_score = "Cao (T·ªët)" if m > g_m * 1.1 else "Th·∫•p (X·∫•u)" if m < g_m * 0.9 else "Trung b√¨nh"

    # Di·ªÖn gi·∫£i logic
    if r_score == "Th·∫•p (T·ªët)" and f_score == "Cao (T·ªët)" and m_score == "Cao (T·ªët)":
        return "üåü Kh√°ch h√†ng VIP/Trung th√†nh"
    elif r_score == "Cao (X·∫•u)" and f_score == "Th·∫•p (X·∫•u)":
        return "‚ö†Ô∏è Kh√°ch h√†ng c√≥ nguy c∆° r·ªùi b·ªè"
    elif r_score == "Th·∫•p (T·ªët)" and f_score == "Th·∫•p (X·∫•u)":
        return "üí° Kh√°ch h√†ng m·ªõi"
    elif f_score == "Cao (T·ªët)":
        return "üíñ Kh√°ch h√†ng th√¢n thi·∫øt"
    elif m_score == "Cao (T·ªët)":
        return "üí∞ Kh√°ch h√†ng chi ti√™u cao"
    elif r_score == "Cao (X·∫•u)":
        return "üí§ Kh√°ch h√†ng ng·ªß ƒë√¥ng"
    
    return "Kh√°ch h√†ng Ti·ªÅm nƒÉng/Trung b√¨nh"

def run_kmeans_segmentation(spark, parquet_path: str):
    from pyspark.ml.feature import VectorAssembler, StandardScaler
    from pyspark.ml.clustering import KMeans
    from pyspark.ml.evaluation import ClusteringEvaluator
    from pyspark.ml import Pipeline

    st.subheader("Ph√¢n c·ª•m kh√°ch h√†ng (KMeans, d·ª±a tr√™n RFM ƒë∆°n gi·∫£n)")

    # ƒê·∫£m b·∫£o Spark s·ªëng & ƒë·ªçc d·ªØ li·ªáu
    if not _spark_is_alive(spark):
        spark = ensure_spark(master)
    app_id = spark.sparkContext.applicationId
    try:
        df = load_parquet_cached(app_id, parquet_path)
    except Exception:
        df = spark.read.parquet(parquet_path)

    # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
    required = ["customer_id", "purchase_ts", "total_amount"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        st.error(f"Thi·∫øu c·ªôt cho RFM: {missing}")
        return

    # Lo·∫°i b·∫£n ghi thi·∫øu timestamp
    df_nonull = df.na.drop(subset=["purchase_ts"])
    if df_nonull.count() == 0:
        st.error("Kh√¥ng c√≥ 'purchase_ts' h·ª£p l·ªá. Ki·ªÉm tra ƒë·ªãnh d·∫°ng th·ªùi gian khi ETL.")
        return

    # T√≠nh RFM
    rfm = (df_nonull.groupBy("customer_id")
           .agg(_max("purchase_ts").alias("last_purchase"),
                count("*").alias("frequency"),
                _sum("total_amount").alias("monetary")))

    max_ts_row = df_nonull.agg(_max("purchase_ts").alias("mx")).collect()
    if not max_ts_row or max_ts_row[0]["mx"] is None:
        st.error("Kh√¥ng t√≠nh ƒë∆∞·ª£c m·ªëc th·ªùi gian l·ªõn nh·∫•t (max_ts).")
        return
    max_ts = max_ts_row[0]["mx"]

    rfm = rfm.withColumn("recency_days", expr(f"datediff(to_timestamp('{str(max_ts)}'), last_purchase)"))
    rfm_clean = rfm.na.drop(subset=["recency_days", "frequency", "monetary"])
    
    if rfm_clean.count() == 0:
        st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu sau khi l√†m s·∫°ch ƒë·ªÉ ph√¢n c·ª•m.")
        return
        
    # L·∫•y trung b√¨nh to√†n c·ª•c ƒë·ªÉ so s√°nh
    try:
        global_avgs = rfm_clean.agg(
            avg("recency_days"), avg("frequency"), avg("monetary")
        ).collect()[0]
        global_r, global_f, global_m = global_avgs[0], global_avgs[1], global_avgs[2]
        if global_r is None: global_r = 0
        if global_f is None: global_f = 0
        if global_m is None: global_m = 0
    except Exception:
        global_r, global_f, global_m = 0, 0, 0


    # Pipeline ti·ªÅn x·ª≠ l√Ω
    features = ["recency_days", "frequency", "monetary"]
    assembler = VectorAssembler(inputCols=features, outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=False)

    # ---- T·ª∞ ƒê·ªòNG CH·ªåN k THEO SILHOUETTE ----
    evaluator = ClusteringEvaluator(featuresCol="features")
    ks = list(range(2, 9))  # d·∫£i k th·ª≠ (2..8)
    scores = []

    # (TƒÉng t·ªëc nh·∫π v·ªõi sample n·∫øu qu√° l·ªõn)
    train_df = rfm_clean
    approx = rfm_clean.count()
    if approx > 200_000:
        train_df = rfm_clean.sample(False, 200_000 / approx, seed=42)

    best_model = None
    best_k = None
    best_score = float("-inf")

    st.markdown("#### 1. Ch·ªçn s·ªë c·ª•m (k) t·ª± ƒë·ªông theo Silhouette")
    progress_bar = st.progress(0, text="ƒêang t√¨m k t·ªët nh·∫•t...")

    for i, k in enumerate(ks):
        model = Pipeline(stages=[assembler, scaler, KMeans(featuresCol="features", k=k, seed=42)]).fit(train_df)
        pred_k = model.transform(train_df)
        sil = evaluator.evaluate(pred_k)
        scores.append((k, sil))
        if sil > best_score:
            best_score, best_k, best_model = sil, k, model
        progress_bar.progress((i + 1) / len(ks), text=f"ƒê√£ th·ª≠ k={k} (Silhouette: {sil:.4f})")
    
    progress_bar.empty()

    # **C·∫¢I TI·∫æN 1: Tr·ª±c quan h√≥a ch·ªçn k**
    scores_df = pd.DataFrame(scores, columns=["k", "silhouette"]).set_index("k")
    st.line_chart(scores_df)
    st.success(f"S·ªë c·ª•m ƒë∆∞·ª£c ch·ªçn: **k = {best_k}** (Silhouette cao nh·∫•t = **{best_score:.4f}**)")

    # D√πng model t·ªët nh·∫•t ƒë·ªÉ g√°n c·ª•m cho TO√ÄN B·ªò d·ªØ li·ªáu rfm_clean
    res = best_model.transform(rfm_clean)

    # L·∫•y profile c·ª•m
    prof_spark = (res.groupBy("prediction")
              .agg(avg("recency_days").alias("avg_recency"),
                   avg("frequency").alias("avg_freq"),
                   avg("monetary").alias("avg_monetary"),
                   count("*").alias("n_customers"))
              .orderBy("prediction"))
    prof_pd = prof_spark.toPandas()


    # **C·∫¢I TI·∫æN 2: D√πng Tab ƒë·ªÉ t·ªï ch·ª©c k·∫øt qu·∫£**
    st.markdown("---")
    st.markdown("#### 2. K·∫øt qu·∫£ ph√¢n c·ª•m")
    
    tab_profile, tab_viz, tab_data = st.tabs([
        "üìä Profile C·ª•m (H·ªç l√† ai?)", 
        "üìà Tr·ª±c quan h√≥a C·ª•m (H·ªç ·ªü ƒë√¢u?)", 
        "üìã D·ªØ li·ªáu chi ti·∫øt"
    ])

    # **C·∫¢I TI·∫æN 3: D√πng st.metric v√† di·ªÖn gi·∫£i c·ª•m**
    with tab_profile:
        st.subheader("Ph√¢n t√≠ch Profile t·ª´ng c·ª•m")
        st.write("""
        D∆∞·ªõi ƒë√¢y l√† ƒë·∫∑c ƒëi·ªÉm trung b√¨nh c·ªßa kh√°ch h√†ng trong t·ª´ng c·ª•m. 
        T√™n c·ª•m (v√≠ d·ª•: "Kh√°ch h√†ng VIP") ƒë∆∞·ª£c t·ª± ƒë·ªông g·ª£i √Ω d·ª±a tr√™n vi·ªác so s√°nh v·ªõi m·ª©c trung b√¨nh chung.
        """)
        
        for idx, row in prof_pd.iterrows():
            cluster_id = row["prediction"]
            # ƒê·ªïi t√™n bi·∫øn 'count' th√†nh 'n_customers' ƒë·ªÉ tr√°nh xung ƒë·ªôt
            avg_r, avg_f, avg_m, n_customers = row["avg_recency"], row["avg_freq"], row["avg_monetary"], row["n_customers"]
            
            # T·ª± ƒë·ªông di·ªÖn gi·∫£i
            title = _interpret_cluster(avg_r, avg_f, avg_m, global_r, global_f, global_m)
            
            st.markdown(f"### C·ª•m {cluster_id}: {title}")
            
            # D√πng st.metric ƒë·ªÉ hi·ªÉn th·ªã ƒë·∫πp m·∫Øt
            c1, c2, c3, c4 = st.columns(4)
            # S·ª≠ d·ª•ng bi·∫øn 'n_customers' m·ªõi
            c1.metric("S·ªë l∆∞·ª£ng KH", f"{n_customers:,.0f} KH")
            c2.metric("Recency (TB)", f"{avg_r:,.1f} ng√†y", 
                      f"{avg_r - global_r:,.1f} vs TB", help=f"Trung b√¨nh chung: {global_r:,.1f} ng√†y")
            c3.metric("Frequency (TB)", f"{avg_f:,.1f} l·∫ßn", 
                      f"{avg_f - global_f:,.1f} vs TB", help=f"Trung b√¨nh chung: {global_f:,.1f} l·∫ßn")
            c4.metric("Monetary (TB)", f"{avg_m:,.0f}", 
                      f"{avg_m - global_m:,.0f} vs TB", help=f"Trung b√¨nh chung: {global_m:,.0f}")
            
            st.divider() # NgƒÉn c√°ch gi·ªØa c√°c c·ª•m

    # **C·∫¢I TI·∫æN 4: Bi·ªÉu ƒë·ªì Scatter Plot R-F-M**
    with tab_viz:
        st.subheader("Tr·ª±c quan h√≥a c√°c c·ª•m (R-F-M)")
        st.write("""
        Bi·ªÉu ƒë·ªì n√†y th·ªÉ hi·ªán v·ªã tr√≠ c·ªßa c√°c kh√°ch h√†ng:
        - **Tr·ª•c X (Recency):** C√†ng v·ªÅ b√™n tr√°i c√†ng t·ªët (m·ªõi mua g·∫ßn ƒë√¢y).
        - **Tr·ª•c Y (Frequency):** C√†ng l√™n cao c√†ng t·ªët (mua nhi·ªÅu l·∫ßn).
        - **K√≠ch th∆∞·ªõc (Size):** C√†ng l·ªõn c√†ng t·ªët (chi ti√™u nhi·ªÅu).
        - **M√†u s·∫Øc (Color):** C·ª•m ƒë∆∞·ª£c g√°n.
        """)
        
        # ======================================================
        # START: S·ª¨A L·ªñI
        # ======================================================
        # L·∫•y m·∫´u d·ªØ li·ªáu ƒë·ªÉ v·∫Ω (tr√°nh crash tr√¨nh duy·ªát n·∫øu c√≥ > 10k ƒëi·ªÉm)
        viz_limit = 5000
        
        # CH·ªåN C√ÅC C·ªòT C·∫¶N THI·∫æT TR∆Ø·ªöC KHI .toPandas()
        # ƒêi·ªÅu n√†y s·∫Ω b·ªè qua c·ªôt timestamp 'last_purchase' v√† c√°c c·ªôt vector
        res_selected = res.select("recency_days", "frequency", "monetary", "prediction")
        
        total_res = res_selected.count() # Count from the selected DF
        if total_res > viz_limit:
            fraction = viz_limit / total_res
            # Now, sample and convert the *selected* DataFrame
            viz_df = res_selected.sample(False, fraction, seed=42).limit(viz_limit).toPandas() 
        else:
            # Convert the *selected* DataFrame
            viz_df = res_selected.toPandas()
        # ======================================================
        # END: S·ª¨A L·ªñI
        # ======================================================

        # Chuy·ªÉn prediction sang string ƒë·ªÉ Streamlit hi·ªÉu l√†
        # c·ªôt ph√¢n lo·∫°i (categorical) cho m√†u s·∫Øc
        viz_df["prediction"] = viz_df["prediction"].astype(str)
        
        if not viz_df.empty:
            st.scatter_chart(
                viz_df,
                x="recency_days",
                y="frequency",
                size="monetary",
                color="prediction",
                use_container_width=True
            )
        else:
            st.warning("Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì scatter.")


    # Tab cu·ªëi c√πng: D·ªØ li·ªáu th√¥ v√† xu·∫•t file
    with tab_data:
        st.subheader("D·ªØ li·ªáu kh√°ch h√†ng chi ti·∫øt (200 m·∫´u)")
        # B·∫£ng k·∫øt qu·∫£ t·ª´ng kh√°ch
        st.dataframe(
            res.select("customer_id", "recency_days", "frequency", "monetary", "prediction")
               .orderBy("prediction", "customer_id")
               .limit(200)
               .toPandas(),
            use_container_width=True
        )

       

# =========================
# ML: Product Recommender using Clustering
# =========================
def build_product_clustering_model(spark, parquet_path: str):
    from pyspark.ml.feature import VectorAssembler, StandardScaler
    from pyspark.ml.clustering import KMeans
    from pyspark.ml.evaluation import ClusteringEvaluator
    from pyspark.ml import Pipeline

    # ƒê·∫£m b·∫£o Spark s·ªëng & ƒë·ªçc d·ªØ li·ªáu
    if not _spark_is_alive(spark):
        spark = ensure_spark(master)
    app_id = spark.sparkContext.applicationId
    try:
        df = load_parquet_cached(app_id, parquet_path)
    except Exception:
        df = spark.read.parquet(parquet_path)

    # Ki·ªÉm tra c·ªôt c·∫ßn thi·∫øt
    required = ["product_category", "product_price", "quantity", "total_amount", "returns"]
    missing = [c for c in required if c not in df.columns]
    if missing:
        st.error(f"Thi·∫øu c·ªôt cho Product Clustering: {missing}")
        return

    # T√≠nh features cho t·ª´ng unique product_category (ƒë·∫°i di·ªán cho s·∫£n ph·∫©m)
    # Features: avg_price, avg_quantity, total_revenue, avg_returns_rate
    prod_features = (df.groupBy("product_category")
                     .agg(
                         avg("product_price").alias("avg_price"),
                         avg("quantity").alias("avg_quantity"),
                         _sum("total_amount").alias("total_revenue"),
                         avg(col("returns").cast("double")).alias("avg_returns_rate")
                     ))

    prod_features_clean = prod_features.na.drop()
    if prod_features_clean.count() == 0:
        st.error("Kh√¥ng ƒë·ªß d·ªØ li·ªáu s·∫£n ph·∫©m sau khi l√†m s·∫°ch ƒë·ªÉ ph√¢n c·ª•m.")
        return

    # Pipeline ti·ªÅn x·ª≠ l√Ω
    features = ["avg_price", "avg_quantity", "total_revenue", "avg_returns_rate"]
    assembler = VectorAssembler(inputCols=features, outputCol="features_raw")
    scaler = StandardScaler(inputCol="features_raw", outputCol="features", withStd=True, withMean=True)

    # Ch·ªçn k t·ª± ƒë·ªông (2-6 v√¨ s·ªë category th∆∞·ªùng √≠t)
    evaluator = ClusteringEvaluator(featuresCol="features")
    ks = list(range(10,20))
    scores = []

    train_df = prod_features_clean  # S·ªë l∆∞·ª£ng √≠t n√™n kh√¥ng c·∫ßn sample

    best_model = None
    best_k = None
    best_score = float("-inf")

    for k in ks:
        model = Pipeline(stages=[assembler, scaler, KMeans(featuresCol="features", k=k, seed=42)]).fit(train_df)
        pred_k = model.transform(train_df)
        sil = evaluator.evaluate(pred_k)
        scores.append((k, sil))
        if sil > best_score:
            best_score, best_k, best_model = sil, k, model

    # Hi·ªÉn th·ªã Silhouette scores
    st.markdown("#### Ch·ªçn s·ªë c·ª•m t·ª± ƒë·ªông theo Silhouette (cho s·∫£n ph·∫©m)")
    st.dataframe(
        pd.DataFrame(scores, columns=["k", "silhouette"]).sort_values("k"),
        use_container_width=True
    )
    st.success(f"S·ªë c·ª•m s·∫£n ph·∫©m ƒë∆∞·ª£c ch·ªçn: **k = {best_k}** (Silhouette = **{best_score:.4f}**)")

    # √Åp d·ª•ng model cho to√†n b·ªô
    clustered_prods = best_model.transform(prod_features_clean)

    # L∆∞u cluster v√†o session state ƒë·ªÉ d√πng cho UI
    st.session_state["clustered_prods"] = clustered_prods
    st.session_state["best_k"] = best_k
    st.session_state["product_model_built"] = True

    st.info("Model ph√¢n c·ª•m s·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng. B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·ªçn s·∫£n ph·∫©m ƒë·ªÉ g·ª£i √Ω b√™n d∆∞·ªõi.")

    # TH√äM: Hi·ªÉn th·ªã n·ªôi dung t·ª´ng c·ª•m
    st.markdown("### N·ªôi dung t·ª´ng c·ª•m s·∫£n ph·∫©m")
    from pyspark.sql.functions import collect_list, size

    # Group by cluster v√† collect list s·∫£n ph·∫©m
    cluster_contents = (clustered_prods
                        .groupBy("prediction")
                        .agg(
                            collect_list("product_category").alias("products"),
                            count("*").alias("num_products")
                        )
                        .orderBy("prediction"))

    cluster_pdf = cluster_contents.toPandas()

    for idx, row in cluster_pdf.iterrows():
        cluster_id = row["prediction"]
        products = row["products"]
        num = row["num_products"]
        st.markdown(f"**C·ª•m {cluster_id}: {num} s·∫£n ph·∫©m**")
        st.write(", ".join(products[:20]))  # Hi·ªÉn th·ªã t·ªëi ƒëa 20 s·∫£n ph·∫©m ƒë·∫ßu ƒë·ªÉ tr√°nh d√†i d√≤ng
        if len(products) > 20:
            st.write(f"... v√† {len(products) - 20} s·∫£n ph·∫©m kh√°c")
        st.markdown("---")

# =========================
# UI
# =========================
st.set_page_config(page_title="E-commerce Big Data App", layout="wide")
st.title("E-commerce Big Data App ‚Äî Spark ETL ‚Ä¢ Analytics ‚Ä¢ ML")

with st.sidebar:
    st.header("K·∫øt n·ªëi")
    master = st.text_input("Spark master", value=DEFAULT_SPARK_MASTER)
    log_level = st.selectbox("Spark log level", ["ERROR","WARN","INFO","DEBUG"], index=1)
    input_path = st.text_input("CSV HDFS path", value=DEFAULT_INPUT)
    parquet_out = st.text_input("Parquet output", value=PARQUET_DIR)
    ts_format = st.text_input("ƒê·ªãnh d·∫°ng th·ªùi gian (Purchase Date)", value="M/d/yyyy H:mm")
    # G·ª£i √Ω: d√πng 'yyyy-MM-dd HH:mm:ss' n·∫øu file c√≥ c·∫£ gi·ªù-ph√∫t-gi√¢y

    if st.button("Kh·ªüi t·∫°o Spark"):
        try:
            st.session_state["spark"] = start_spark(master=master, log_level=log_level)
            st.success("SparkSession ƒë√£ s·∫µn s√†ng.")
        except Exception as e:
            st.error(f"Kh·ªüi t·∫°o Spark th·∫•t b·∫°i: {e}")
            st.code(traceback.format_exc())

    if st.button("Restart SparkSession"):
        try:
            old = st.session_state.get("spark")
            if old is not None:
                try: old.stop()
                except Exception: pass
            _hard_reset_spark_jvm()
            # Xo√° cache resource g·∫Øn v·ªõi appId c≈© (n·∫øu c√≥)
            try: st.cache_resource.clear()
            except Exception: pass
            st.session_state["spark"] = start_spark(master=master, log_level=log_level)
            st.success("ƒê√£ restart SparkSession.")
        except Exception as e:
            st.error(f"Restart Spark th·∫•t b·∫°i: {e}")
            st.code(traceback.format_exc())


spark = st.session_state.get("spark")
if spark is None:
    st.warning("H√£y kh·ªüi t·∫°o Spark ·ªü thanh b√™n.")
    st.stop()

st.markdown("### 1) ETL")
if st.button("Ch·∫°y ETL t·ª´ CSV ‚Üí Parquet"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        out = run_etl(spark, src_path=input_path, dst_parquet=parquet_out, ts_format=ts_format.strip())
        st.session_state["parquet_path"] = out
    except Exception as e:
        st.error(f"L·ªói ETL: {e}")
        st.code(traceback.format_exc())

st.markdown("### 2) Analytics")
ppath = st.text_input("Parquet ƒë√£ ETL", value=st.session_state.get("parquet_path", PARQUET_DIR))
if st.button("Ch·∫°y Analytics"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        run_analytics(spark, ppath)
    except Exception as e:
        st.error(f"L·ªói Analytics: {e}")
        st.code(traceback.format_exc())

st.markdown("### 3) Ph√¢n t√≠ch n√¢ng cao")
if st.button("üìä Ph√¢n t√≠ch xu h∆∞·ªõng mua s·∫Øm"):
    try:
        spark = start_spark(log_level=log_level)
        df = spark.read.parquet(ppath)
        run_trend_analysis(df)
    except Exception as e:
        st.error(f"L·ªói khi ph√¢n t√≠ch xu h∆∞·ªõng: {e}")
        st.code(traceback.format_exc())

if st.button("üß† Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng"):
    try:
        spark = start_spark(log_level=log_level)
        df = spark.read.parquet(ppath)
        run_customer_behavior(df)
    except Exception as e:
        st.error(f"L·ªói khi ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng: {e}")
        st.code(traceback.format_exc())

st.markdown("### 4) Machine Learning")

if st.button("Ph√¢n c·ª•m kh√°ch h√†ng (KMeans)"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        run_kmeans_segmentation(spark, ppath)
    except Exception as e:
        st.error(f"L·ªói ML (KMeans): {e}")
        st.code(traceback.format_exc())

# Ph·∫ßn g·ª£i √Ω s·∫£n ph·∫©m: T√°ch build model v√† UI
st.markdown("### üõí G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n Clustering")
if st.button("X√¢y d·ª±ng model ph√¢n c·ª•m s·∫£n ph·∫©m"):
    try:
        spark = ensure_spark(master, log_level=log_level)
        build_product_clustering_model(spark, ppath)
    except Exception as e:
        st.error(f"L·ªói x√¢y d·ª±ng model: {e}")
        st.code(traceback.format_exc())

# UI ch·ªçn v√† g·ª£i √Ω: Lu√¥n hi·ªÉn th·ªã n·∫øu model ƒë√£ build
if st.session_state.get("product_model_built", False):
    st.subheader("üõí G·ª£i √Ω s·∫£n ph·∫©m d·ª±a tr√™n Clustering (theo Product Category)")

    # L·∫•y unique categories t·ª´ data (c·∫ßn ƒë·ªçc data ƒë·ªÉ c√≥ list)
    try:
        spark_temp = ensure_spark(master, log_level=log_level)
        df_temp = spark_temp.read.parquet(ppath)
        unique_cats = [row["product_category"] for row in df_temp.select("product_category").distinct().collect()]
    except Exception:
        unique_cats = []
        st.warning("Kh√¥ng th·ªÉ t·∫£i danh s√°ch s·∫£n ph·∫©m. H√£y ch·∫°y ETL tr∆∞·ªõc.")

    if unique_cats:
        selected_category = st.selectbox("Ch·ªçn Product Category (s·∫£n ph·∫©m ƒë·∫°i di·ªán):", unique_cats, key="select_product")

        if st.button("T√¨m g·ª£i √Ω s·∫£n ph·∫©m t∆∞∆°ng t·ª±", key="find_recommend"):
            # T√¨m cluster c·ªßa s·∫£n ph·∫©m ƒë∆∞·ª£c ch·ªçn
            selected_cluster = (st.session_state["clustered_prods"]
                                .filter(col("product_category") == selected_category)
                                .select("prediction").collect()[0]["prediction"])

            # L·∫•y top 5 s·∫£n ph·∫©m kh√°c trong c√πng cluster (theo total_revenue)
            recommendations = (st.session_state["clustered_prods"]
                               .filter(col("prediction") == selected_cluster)
                               .filter(col("product_category") != selected_category)
                               .select("product_category", "avg_price", "avg_quantity", "total_revenue", "avg_returns_rate")
                               .orderBy(F.desc("total_revenue"))
                               .limit(5))

            if recommendations.count() > 0:
                rec_pdf = recommendations.toPandas()
                st.markdown(f"**G·ª£i √Ω cho '{selected_category}' (C·ª•m {selected_cluster})**")
                st.dataframe(
                    rec_pdf,
                    use_container_width=True,
                    column_config={
                        "product_category": "S·∫£n ph·∫©m g·ª£i √Ω",
                        "avg_price": "Gi√° TB",
                        "avg_quantity": "S·ªë l∆∞·ª£ng TB",
                        "total_revenue": "Doanh thu t·ªïng",
                        "avg_returns_rate": "T·ªâ l·ªá tr·∫£ TB"
                    }
                )
            else:
                st.warning("Kh√¥ng c√≥ s·∫£n ph·∫©m t∆∞∆°ng t·ª± trong c·ª•m n√†y.")
    else:
        st.warning("Kh√¥ng c√≥ d·ªØ li·ªáu s·∫£n ph·∫©m ƒë·ªÉ ch·ªçn.")
else:
    st.info("Nh·∫•n 'X√¢y d·ª±ng model ph√¢n c·ª•m s·∫£n ph·∫©m' ƒë·ªÉ b·∫Øt ƒë·∫ßu.")