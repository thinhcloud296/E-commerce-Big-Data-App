# Ph√¢n t√≠ch D·ªØ li·ªáu Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠ ‚Äî Spark + HDFS + Streamlit

D·ª± √°n Big Data nh·ªè, ho√†n ch·ªânh t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi, d√πng ƒë·ªÉ ph√¢n t√≠ch h√†nh vi kh√°ch h√†ng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠. Bao g·ªìm:

* **ETL v·ªõi Apache Spark** (ƒë·ªçc CSV ‚Üí l√†m s·∫°ch ‚Üí ghi Parquet ph√¢n v√πng theo ng√†y)
* **B·∫£ng ƒëi·ªÅu khi·ªÉn ph√¢n t√≠ch (Analytics)**: KPI, doanh thu theo ng√†y, danh m·ª•c v√† ph∆∞∆°ng th·ª©c thanh to√°n ph·ªï bi·∫øn
* **Ph√¢n c·ª•m kh√°ch h√†ng (KMeans - RFM)** t·ª± ƒë·ªông ch·ªçn s·ªë c·ª•m b·∫±ng Silhouette + b·∫£ng h·ªì s∆° c·ª•m
* **L∆∞u tr·ªØ d·ªØ li·ªáu tr√™n HDFS** (ho·∫∑c ch·∫°y tr√™n file local) v√† giao di·ªán **Streamlit**

> T√†i li·ªáu n√†y h∆∞·ªõng d·∫´n **2 c√°ch ch·∫°y d·ª± √°n**: b·∫±ng **Docker (ƒë·ªÅ xu·∫•t)** ho·∫∑c **Conda (local)**. B·∫°n c√≥ th·ªÉ ch·ªçn m·ªôt trong hai c√°ch.

---

## 0) C·∫•u tr√∫c d·ª± √°n

```
E:/DoAnBigData/
‚îú‚îÄ main.py                 # ·ª®ng d·ª•ng Streamlit (ETL + Analytics + KMeans)
‚îú‚îÄ requirements.txt        # Danh s√°ch th∆∞ vi·ªán pip
‚îú‚îÄ environment.yml         # C·∫•u h√¨nh Conda (t√πy ch·ªçn)
‚îî‚îÄ ecommerce_data.csv      # File d·ªØ li·ªáu ƒë·∫ßu v√†o m·∫´u
```

---

## A) Ch·∫°y b·∫±ng Docker (Khuy·∫øn ngh·ªã)

Y√™u c·∫ßu c√≥ s·∫µn c·ª•m Spark + Hadoop + container `jupyter_app` trong c√πng m·∫°ng Docker.

### A1) Kh·ªüi ƒë·ªông c·ª•m

```bash
docker compose up -d spark-master spark-worker namenode datanode jupyter_app

docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

* **Jupyter**: [http://localhost:8888](http://localhost:8888)
* **Streamlit**: [http://localhost:8501](http://localhost:8501)

### A2) C√†i ƒë·∫∑t th∆∞ vi·ªán Python trong container `jupyter_app`

```bash
docker exec -it jupyter_app bash -lc "pip install -U pip && pip install -r /home/jovyan/work/requirements.txt"
# N·∫øu ch∆∞a c√≥ requirements.txt: pip install streamlit pandas matplotlib pyspark==3.3.0
```

### A3) T·∫£i d·ªØ li·ªáu l√™n HDFS

```bash
docker exec -it jupyter_app bash -lc "\
  hdfs dfs -mkdir -p /input && \
  hdfs dfs -put -f /home/jovyan/work/ecommerce_data.csv /input/ && \
  hdfs dfs -ls /input"
```

### A4) Ch·∫°y ·ª©ng d·ª•ng Streamlit

```bash
docker exec -it jupyter_app bash -lc "cd /home/jovyan/work && python -m streamlit run main.py --server.address=0.0.0.0 --server.port=8501"
```

Sau ƒë√≥ m·ªü [http://localhost:8501](http://localhost:8501)

### A5) C·∫•u h√¨nh sidebar (Docker mode)

* **Spark master**: `spark://spark-master:7077`
* **CSV HDFS path**: `hdfs://namenode:8020/input/ecommerce_data.csv`
* **Parquet output**: `hdfs://namenode:8020/warehouse/ecommerce_parquet`
* **ƒê·ªãnh d·∫°ng th·ªùi gian**: `M/d/yyyy H:mm` (v√≠ d·ª•: `9/8/2020 9:38`)

### A6) Quy tr√¨nh thao t√°c

1. Nh·∫•n **Kh·ªüi t·∫°o SparkSession** (Restart Spark n·∫øu c·∫ßn)
2. Ch·∫°y **ETL** (ƒë·ªçc CSV, chu·∫©n h√≥a, ghi Parquet theo ng√†y)
3. Ch·∫°y **Analytics** (hi·ªÉn th·ªã KPI, bi·ªÉu ƒë·ªì, top category/payment)
4. Ch·∫°y **Ph√¢n c·ª•m kh√°ch h√†ng (KMeans)** ‚Äî t·ª± ƒë·ªông ch·ªçn k t·ªët nh·∫•t, hi·ªÉn th·ªã:

   * B·∫£ng ƒëi·ªÉm Silhouette theo k
   * Danh s√°ch kh√°ch h√†ng & c·ª•m
   * B·∫£ng **H·ªì s∆° c·ª•m (Cluster Profile)**: trung b√¨nh R/F/M, s·ªë kh√°ch h√†ng

---

## B) Ch·∫°y local b·∫±ng Conda (kh√¥ng c·∫ßn Docker)

D√†nh cho demo ƒë∆°n gi·∫£n kh√¥ng d√πng HDFS.

### B1) T·∫°o m√¥i tr∆∞·ªùng

```bash
conda create -n ecom-bigdata python=3.10 -y
conda activate ecom-bigdata
conda install -c conda-forge openjdk=8 -y
pip install -U pip
pip install -r requirements.txt
```

> N·∫øu b·ªã l·ªói Java, th√™m bi·∫øn m√¥i tr∆∞·ªùng trong PowerShell:

```powershell
$env:JAVA_HOME = $env:CONDA_PREFIX
$env:PATH = "$env:JAVA_HOME\Library\bin;$env:JAVA_HOME\bin;$env:PATH"
java -version
```

### B2) Ch·∫°y ·ª©ng d·ª•ng

```bash
cd E:/DoAnBigData
streamlit run main.py
```

### B3) C·∫•u h√¨nh sidebar (local)

* **Spark master**: `local[*]`
* **CSV path**: `file:///E:/DoAnBigData/ecommerce_data.csv`
* **Parquet output**: `file:///E:/DoAnBigData/ecommerce_parquet`
* **ƒê·ªãnh d·∫°ng th·ªùi gian**: `M/d/yyyy H:mm`

---

## C) C√†i ƒë·∫∑t Docker Desktop

N·∫øu b·∫°n ch∆∞a c√≥ Docker, l√†m theo c√°c b∆∞·ªõc sau ƒë·ªÉ ch·∫°y ƒë∆∞·ª£c c·ª•m Spark + HDFS + Streamlit.

### C1) T·∫£i v√† c√†i Docker Desktop

1. M·ªü: [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)
2. Ch·ªçn ƒë√∫ng h·ªá ƒëi·ªÅu h√†nh:

   * **Windows 10/11** (64-bit) ‚Üí Docker Desktop for Windows *(y√™u c·∫ßu WSL2)*
   * **macOS (Intel/M1/M2)** ‚Üí Docker Desktop for Mac
   * **Linux** ‚Üí c√†i `docker` v√† `docker-compose` qua terminal theo distro
3. C√†i xong, m·ªü Docker Desktop v√† ch·ªù bi·ªÉu t∆∞·ª£ng c√° voi üê≥ b√°o **Running**.
4. Ki·ªÉm tra trong terminal:

   ```bash
   docker --version
   docker compose version
   ```

### C2) C·∫•u h√¨nh WSL2 (Windows)

1. PowerShell (Run as Administrator):

   ```bash
   wsl --install
   ```

   Kh·ªüi ƒë·ªông l·∫°i m√°y n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu.
2. ƒê·∫∑t WSL2 m·∫∑c ƒë·ªãnh:

   ```bash
   wsl --set-default-version 2
   ```
3. Docker Desktop ‚Üí **Settings ‚Üí General**: b·∫≠t

   * *Use the WSL 2 based engine*
   * *Start Docker Desktop when you log in*
4. **Settings ‚Üí Resources ‚Üí WSL Integration**: b·∫≠t t√≠ch cho distro (v√≠ d·ª• **Ubuntu**).

### C3) Ch·∫°y th·ª≠ container m·∫´u

```bash
docker run hello-world
```

N·∫øu in th√¥ng ƒëi·ªáp *Hello from Docker!* ‚Üí Docker ƒë√£ ho·∫°t ƒë·ªông ƒë√∫ng.

### C4) Ch·∫°y c·ª•m Hadoop + Spark + Jupyter c·ªßa d·ª± √°n

T·∫°i th∆∞ m·ª•c d·ª± √°n c√≥ `docker-compose.yml`:

```bash
docker compose up -d
```

C√°c d·ªãch v·ª• s·∫Ω kh·ªüi ch·∫°y: **namenode, datanode, spark-master, spark-worker, jupyter_app**.

---

## T√≠nh nƒÉng ch√≠nh

* L√†m s·∫°ch d·ªØ li·ªáu, x·ª≠ l√Ω null, chuy·ªÉn ƒë·ªïi th·ªùi gian, ghi Parquet ph√¢n v√πng theo ng√†y.
* Th·ªëng k√™ & bi·ªÉu ƒë·ªì: doanh thu, ƒë∆°n h√†ng, AOV, danh m·ª•c, ph∆∞∆°ng th·ª©c thanh to√°n.
* Ph√¢n c·ª•m kh√°ch h√†ng theo RFM, ch·ªçn s·ªë c·ª•m t·ª± ƒë·ªông b·∫±ng Silhouette.
* Cache d·ªØ li·ªáu Parquet gi√∫p ch·∫°y l·∫°i nhanh h∆°n.

---

## Gi·∫£i th√≠ch k·∫øt qu·∫£ KMeans

* **recency_days**: s·ªë ng√†y k·ªÉ t·ª´ l·∫ßn mua g·∫ßn nh·∫•t ‚Üí c√†ng nh·ªè c√†ng m·ªõi.
* **frequency**: s·ªë l·∫ßn mua h√†ng.
* **monetary**: t·ªïng chi ti√™u.
* **prediction**: s·ªë c·ª•m (0..k-1)

V√≠ d·ª• ph√¢n lo·∫°i:

| ƒê·∫∑c ƒëi·ªÉm                                   | Nh√≥m kh√°ch h√†ng          |
| ------------------------------------------ | ------------------------ |
| Recency th·∫•p, Frequency cao, Monetary cao  | VIP / Trung th√†nh        |
| Recency th·∫•p, Frequency th·∫•p               | Kh√°ch m·ªõi                |
| Recency cao, Frequency th·∫•p, Monetary th·∫•p | Kh√¥ng ho·∫°t ƒë·ªông / R·ªùi b·ªè |

---

## Kh·∫Øc ph·ª•c l·ªói th∆∞·ªùng g·∫∑p

### 1) `UnknownHostException: namenode`

Ch·∫°y local nh∆∞ng d√πng ƒë∆∞·ªùng d·∫´n HDFS ‚Üí h√£y ƒë·ªïi th√†nh `file:///...` ho·∫∑c ch·∫°y trong Docker.

### 2) `Cannot call methods on a stopped SparkContext`

Spark b·ªã kh·ªüi ƒë·ªông l·∫°i, cache c≈© c√≤n gi·ªØ. Nh·∫•n **Restart SparkSession** r·ªìi th·ª≠ l·∫°i.

### 3) C·∫£nh b√°o m√†u v√†ng: l·ªói parse th·ªùi gian

Nh·∫≠p ƒë√∫ng ƒë·ªãnh d·∫°ng th·ªùi gian: `M/d/yyyy H:mm`, `yyyy-MM-dd HH:mm:ss`, `dd/MM/yyyy`.

### 4) `streamlit: command not found`

Container b·ªã reset ‚Üí c√†i l·∫°i:

```bash
docker exec -it jupyter_app bash -lc "pip install -U pip && pip install -r /home/jovyan/work/requirements.txt"
```

---

## Y√™u c·∫ßu m√¥i tr∆∞·ªùng

* **Docker mode**: Docker + Docker Compose, c·ª•m Spark/Hadoop + `jupyter_app`
* **Local mode**: Python 3.10, PySpark 3.3.0, Java 8, Streamlit, pandas, matplotlib

### requirements.txt

```
streamlit
pyspark==3.3.0
pandas
matplotlib
```

### environment.yml

```yaml
name: ecom-bigdata
channels:
  - conda-forge
dependencies:
  - python=3.10
  - openjdk=8
  - pip
  - pip:
      - streamlit
      - pyspark==3.3.0
      - pandas
      - matplotlib
```

---

## B·∫£n quy·ªÅn

S·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p v√† nghi√™n c·ª©u.
