# PhÃ¢n tÃ­ch Dá»¯ liá»‡u ThÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ â€” Spark + HDFS + Streamlit

Dá»± Ã¡n Big Data nhá», hoÃ n chá»‰nh tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i, dÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch hÃ nh vi khÃ¡ch hÃ ng thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­. Bao gá»“m:

* **ETL vá»›i Apache Spark** (Ä‘á»c CSV â†’ lÃ m sáº¡ch â†’ ghi Parquet phÃ¢n vÃ¹ng theo ngÃ y)
* **Báº£ng Ä‘iá»u khiá»ƒn phÃ¢n tÃ­ch (Analytics)**: KPI, doanh thu theo ngÃ y, danh má»¥c vÃ  phÆ°Æ¡ng thá»©c thanh toÃ¡n phá»• biáº¿n
* **PhÃ¢n cá»¥m khÃ¡ch hÃ ng (KMeans - RFM)** tá»± Ä‘á»™ng chá»n sá»‘ cá»¥m báº±ng Silhouette + báº£ng há»“ sÆ¡ cá»¥m
* **LÆ°u trá»¯ dá»¯ liá»‡u trÃªn HDFS** (hoáº·c cháº¡y trÃªn file local) vÃ  giao diá»‡n **Streamlit**

> TÃ i liá»‡u nÃ y hÆ°á»›ng dáº«n **2 cÃ¡ch cháº¡y dá»± Ã¡n**: báº±ng **Docker (Ä‘á» xuáº¥t)** hoáº·c **Conda (local)**. Báº¡n cÃ³ thá»ƒ chá»n má»™t trong hai cÃ¡ch.

---

## 0) Cáº¥u trÃºc dá»± Ã¡n

```
E:/DoAnBigData/
â”œâ”€ main.py                 # á»¨ng dá»¥ng Streamlit (ETL + Analytics + KMeans)
â”œâ”€ requirements.txt        # Danh sÃ¡ch thÆ° viá»‡n pip
â”œâ”€ environment.yml         # Cáº¥u hÃ¬nh Conda (tÃ¹y chá»n)
â””â”€ ecommerce_data.csv      # File dá»¯ liá»‡u Ä‘áº§u vÃ o máº«u
```

---

## A) Cháº¡y báº±ng Docker + Jupyter (Khuyáº¿n nghá»‹)

Sá»­ dá»¥ng **Jupyter** lÃ m Ä‘iá»ƒm Ä‘iá»u khiá»ƒn chÃ­nh (má»Ÿ terminal bÃªn trong Jupyter Ä‘á»ƒ cháº¡y Streamlit). Cá»¥m Spark + HDFS + `jupyter_app` cÃ¹ng máº¡ng Docker.

### A1) Khá»Ÿi Ä‘á»™ng cá»¥m

```bash
docker compose up -d
```

* **Jupyter (Lab)**: [http://localhost:8888](http://localhost:8888)

  * Token Ä‘Äƒng nháº­p : **bigdata**
* **Streamlit**: sáº½ cháº¡y **bÃªn trong** Jupyter (qua Terminal) vÃ  lá»™ cá»•ng [http://localhost:8501](http://localhost:8501)

### A2) Má»Ÿ Jupyter vÃ  táº¡o Terminal

1. VÃ o [http://localhost:8888](http://localhost:8888) â†’ nháº­p token **bigdata** â†’ vÃ o **JupyterLab**.
2. á» Launcher (mÃ n hÃ¬nh chÃ­nh), báº¥m **Terminal** (hoáº·c `File â†’ New â†’ Terminal`).

### A3) Táº£i dataset lÃªn HDFS (cháº¡y ngay trong Terminal Jupyter)

```bash
hdfs dfs -mkdir -p /input
hdfs dfs -put -f /home/jovyan/work/ecommerce_data.csv /input/
hdfs dfs -ls /input
```

### A4) Cháº¡y á»©ng dá»¥ng Streamlit tá»« Terminal Jupyter

```bash
cd /home/jovyan/work
python -m streamlit run main.py --server.address=0.0.0.0 --server.port=8501
```

Má»Ÿ trÃ¬nh duyá»‡t: **[http://localhost:8501](http://localhost:8501)**

### A5) Cáº¥u hÃ¬nh sidebar (Jupyter + Docker)

* **Spark master**: `spark://spark-master:7077`
* **CSV HDFS path**: `hdfs://namenode:8020/input/ecommerce_data.csv`
* **Parquet output**: `hdfs://namenode:8020/warehouse/ecommerce_parquet`
* **Äá»‹nh dáº¡ng thá»i gian**: `M/d/yyyy H:mm`  (vÃ­ dá»¥: `9/8/2020 9:38`)

### A6) Quy trÃ¬nh thao tÃ¡c

1. **Khá»Ÿi táº¡o/Restart SparkSession** á»Ÿ sidebar.
2. **Cháº¡y ETL** (CSV â†’ lÃ m sáº¡ch â†’ Parquet partition theo `purchase_date`, ghi Ä‘Ã¨).
3. **Cháº¡y Analytics** (KPI, biá»ƒu Ä‘á»“).
4. **PhÃ¢n cá»¥m KMeans** (auto chá»n k theo Silhouette) â†’ xem **báº£ng Ä‘iá»ƒm Silhouette**, **danh sÃ¡ch khÃ¡ch hÃ ng theo cá»¥m** vÃ  **Cluster Profile**.

> **Máº¹o:** Náº¿u báº¡n thÃ­ch thao tÃ¡c má»i thá»© ngoÃ i Jupyter, cÃ³ thá»ƒ dÃ¹ng lá»‡nh `docker exec` tÆ°Æ¡ng Ä‘Æ°Æ¡ng tá»« host. Tuy nhiÃªn, cÃ¡ch á»Ÿ trÃªn giÃºp báº¡n â€œtáº¥t cáº£ trong Jupyterâ€.

---

## C) CÃ i Ä‘áº·t Docker Desktop

Náº¿u báº¡n chÆ°a cÃ³ Docker, lÃ m theo cÃ¡c bÆ°á»›c sau Ä‘á»ƒ cháº¡y Ä‘Æ°á»£c cá»¥m Spark + HDFS + Streamlit.

### C1) Táº£i vÃ  cÃ i Docker Desktop

1. Má»Ÿ: [https://www.docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop)
2. Chá»n Ä‘Ãºng há»‡ Ä‘iá»u hÃ nh:

   * **Windows 10/11** (64-bit) â†’ Docker Desktop for Windows *(yÃªu cáº§u WSL2)*
   * **macOS (Intel/M1/M2)** â†’ Docker Desktop for Mac
   * **Linux** â†’ cÃ i `docker` vÃ  `docker-compose` qua terminal theo distro
3. CÃ i xong, má»Ÿ Docker Desktop vÃ  chá» biá»ƒu tÆ°á»£ng cÃ¡ voi ğŸ³ bÃ¡o **Running**.
4. Kiá»ƒm tra trong terminal:

   ```bash
   docker --version
   docker compose version
   ```

### C2) Cáº¥u hÃ¬nh WSL2 (Windows)

1. PowerShell (Run as Administrator):

   ```bash
   wsl --install
   ```

   Khá»Ÿi Ä‘á»™ng láº¡i mÃ¡y náº¿u Ä‘Æ°á»£c yÃªu cáº§u.
2. Äáº·t WSL2 máº·c Ä‘á»‹nh:

   ```bash
   wsl --set-default-version 2
   ```
3. Docker Desktop â†’ **Settings â†’ General**: báº­t

   * *Use the WSL 2 based engine*
   * *Start Docker Desktop when you log in*
4. **Settings â†’ Resources â†’ WSL Integration**: báº­t tÃ­ch cho distro (vÃ­ dá»¥ **Ubuntu**).

### C3) Cháº¡y thá»­ container máº«u

```bash
docker run hello-world
```

Náº¿u in thÃ´ng Ä‘iá»‡p *Hello from Docker!* â†’ Docker Ä‘Ã£ hoáº¡t Ä‘á»™ng Ä‘Ãºng.

### C4) Cháº¡y cá»¥m Hadoop + Spark + Jupyter cá»§a dá»± Ã¡n

Táº¡i thÆ° má»¥c dá»± Ã¡n cÃ³ `docker-compose.yml`:

```bash
docker compose up -d
```

CÃ¡c dá»‹ch vá»¥ sáº½ khá»Ÿi cháº¡y: **namenode, datanode, spark-master, spark-worker, jupyter_app**.

---

## TÃ­nh nÄƒng chÃ­nh

* LÃ m sáº¡ch dá»¯ liá»‡u, xá»­ lÃ½ null, chuyá»ƒn Ä‘á»•i thá»i gian, ghi Parquet phÃ¢n vÃ¹ng theo ngÃ y.
* Thá»‘ng kÃª & biá»ƒu Ä‘á»“: doanh thu, Ä‘Æ¡n hÃ ng, AOV, danh má»¥c, phÆ°Æ¡ng thá»©c thanh toÃ¡n.
* PhÃ¢n cá»¥m khÃ¡ch hÃ ng theo RFM, chá»n sá»‘ cá»¥m tá»± Ä‘á»™ng báº±ng Silhouette.
* Cache dá»¯ liá»‡u Parquet giÃºp cháº¡y láº¡i nhanh hÆ¡n.

---

## Giáº£i thÃ­ch káº¿t quáº£ KMeans

* **recency_days**: sá»‘ ngÃ y ká»ƒ tá»« láº§n mua gáº§n nháº¥t â†’ cÃ ng nhá» cÃ ng má»›i.
* **frequency**: sá»‘ láº§n mua hÃ ng.
* **monetary**: tá»•ng chi tiÃªu.
* **prediction**: sá»‘ cá»¥m (0..k-1)

VÃ­ dá»¥ phÃ¢n loáº¡i:

| Äáº·c Ä‘iá»ƒm                                   | NhÃ³m khÃ¡ch hÃ ng          |
| ------------------------------------------ | ------------------------ |
| Recency tháº¥p, Frequency cao, Monetary cao  | VIP / Trung thÃ nh        |
| Recency tháº¥p, Frequency tháº¥p               | KhÃ¡ch má»›i                |
| Recency cao, Frequency tháº¥p, Monetary tháº¥p | KhÃ´ng hoáº¡t Ä‘á»™ng / Rá»i bá» |

---

## Kháº¯c phá»¥c lá»—i thÆ°á»ng gáº·p

### 1) `UnknownHostException: namenode`

Cháº¡y local nhÆ°ng dÃ¹ng Ä‘Æ°á»ng dáº«n HDFS â†’ hÃ£y Ä‘á»•i thÃ nh `file:///...` hoáº·c cháº¡y trong Docker.

### 2) `Cannot call methods on a stopped SparkContext`

Spark bá»‹ khá»Ÿi Ä‘á»™ng láº¡i, cache cÅ© cÃ²n giá»¯. Nháº¥n **Restart SparkSession** rá»“i thá»­ láº¡i.

### 3) Cáº£nh bÃ¡o mÃ u vÃ ng: lá»—i parse thá»i gian

Nháº­p Ä‘Ãºng Ä‘á»‹nh dáº¡ng thá»i gian: `M/d/yyyy H:mm`, `yyyy-MM-dd HH:mm:ss`, `dd/MM/yyyy`.

### 4) `streamlit: command not found`

Container bá»‹ reset â†’ cÃ i láº¡i:

```bash
docker exec -it jupyter_app bash -lc "pip install -U pip && pip install -r /home/jovyan/work/requirements.txt"
```

---

## YÃªu cáº§u mÃ´i trÆ°á»ng

* **Docker mode**: Docker + Docker Compose, cá»¥m Spark/Hadoop + `jupyter_app`
* **Local mode**: Python 3.10, PySpark 3.3.0, Java 8, Streamlit, pandas, matplotlib

### requirements.txt

```
streamlit
pyspark==3.3.0
pandas
matplotlib
```

---

## Báº£n quyá»n

Sá»­ dá»¥ng cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.
